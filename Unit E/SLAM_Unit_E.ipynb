{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "348a3e49f15e5b740654d37159b4f4a2",
     "grade": false,
     "grade_id": "cell-414f94f9c97f9ca5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# SLAM Unit E\n",
    "In this unit, you will learn about the particle filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "85302f004c6e52d58d32200efec34f71",
     "grade": false,
     "grade_id": "cell-23ee6cd75a12ad2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Why we would want another representation for the belief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"//av.tib.eu/player/49017\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc0e08e8890>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't see a video below, run this cell.\n",
    "# YouTube = True  # Uncomment to get YouTube videos instead of TIB AV.\n",
    "from IPython.display import IFrame\n",
    "IFrame(\"https://www.youtube.com/embed/ME7kUbLYMno\" if \"YouTube\" in globals() else \"//av.tib.eu/player/49017\",\n",
    "       width=560, height=315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "621f6960d26a4c53b5c3bc58c2e04cba",
     "grade": false,
     "grade_id": "cell-c14565d3fae51e34",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Which alternative is the correct solution, \"A\", \"B\", or \"C\"?\n",
    "bayes_can_handle = \"C\"\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "685bbe88946e961e37d687e159656d0d",
     "grade": true,
     "grade_id": "cell-4fbacc50b69be8f7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from hashlib import shake_128\n",
    "def public_string_test(the_answer_string, reference):\n",
    "    m = shake_128()\n",
    "    m.update(the_answer_string.encode())\n",
    "    return m.hexdigest(4) == reference\n",
    "assert public_string_test(\"bayes_says=\"+bayes_can_handle.strip().lower(), 'ebd1fdee'), \"Oh no, your answer is wrong!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "063b72376104aef54a0f3aead3da8277",
     "grade": false,
     "grade_id": "cell-a7207b53623a4b9d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"//av.tib.eu/player/49018\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc0dd079e50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't see a video below, run this cell.\n",
    "from IPython.display import IFrame\n",
    "IFrame(\"https://www.youtube.com/embed/N6-1aGWE9IM\" if \"YouTube\" in globals() else \"//av.tib.eu/player/49018\",\n",
    "       width=560, height=315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a9d16cb3b30fb8613fbbdec7aed3a89",
     "grade": false,
     "grade_id": "cell-18b7156a45e10387",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Is it possible to model the belief using a very wide normal distribution, \"Yes\" or \"No\"?\n",
    "wide_normal_possible = \"Yes\"\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3655e7349a3afb70253ff67f7c502e93",
     "grade": true,
     "grade_id": "cell-e62d1cb910f9dee6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from hashlib import shake_128\n",
    "def public_string_test(the_answer_string, reference):\n",
    "    m = shake_128()\n",
    "    m.update(the_answer_string.encode())\n",
    "    return m.hexdigest(4) == reference\n",
    "assert public_string_test(\"it is=\"+wide_normal_possible.strip().lower(), 'b41dfcdc'), \"Oh no, your answer is wrong!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a19e6f51bf4c549791ddf084a94c52e",
     "grade": false,
     "grade_id": "cell-2fe6a81b060d7ded",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"//av.tib.eu/player/49019\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc0dd061090>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't see a video below, run this cell.\n",
    "from IPython.display import IFrame\n",
    "IFrame(\"https://www.youtube.com/embed/dgvi7eLleOo\" if \"YouTube\" in globals() else \"//av.tib.eu/player/49019\",\n",
    "       width=560, height=315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16d68adfb699d583a81f7237af415fc8",
     "grade": false,
     "grade_id": "cell-7f2e88d513b7d969",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Can we use the \"large variance initialization\" trick with the Kalman filter localization\n",
    "# approach we developed in the previous unit? Answer with \"A\", \"B\", or \"C\".\n",
    "wide_normal_possible_too = \"C\"\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7dd7dd343309a15ffea922d2ae542f39",
     "grade": true,
     "grade_id": "cell-3b123d4404901659",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from hashlib import shake_128\n",
    "def public_string_test(the_answer_string, reference):\n",
    "    m = shake_128()\n",
    "    m.update(the_answer_string.encode())\n",
    "    return m.hexdigest(4) == reference\n",
    "assert public_string_test(\"maybe=\"+wide_normal_possible_too.strip().lower(), 'f84fab62'), \"Oh no, your answer is wrong!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01e568e222e1ef5ef65b33a7f5af8d54",
     "grade": false,
     "grade_id": "cell-b033de2edf5d6482",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introducing the particle filter: the prediction step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3a02ac5993a652c0cc3d8d1a0127d1a",
     "grade": false,
     "grade_id": "cell-85e2d0a59de08d4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"//av.tib.eu/player/49020\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc0dd02e990>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't see a video below, run this cell.\n",
    "from IPython.display import IFrame\n",
    "IFrame(\"https://www.youtube.com/embed/9jhnye7I2pU\" if \"YouTube\" in globals() else \"//av.tib.eu/player/49020\",\n",
    "       width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b789d284edf68d083775148219a73c92",
     "grade": false,
     "grade_id": "cell-9180b54f081d97c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Programming assignment: program the prediction step of the particle filter (15 Points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bac524da3256b539e311e8b959e7f67f",
     "grade": false,
     "grade_id": "cell-0888dab1645cfb84",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The particle filter, prediciton only.\n",
    "#\n",
    "# slam_08_a_particle_prediciton.\n",
    "from lego_robot import *\n",
    "from math import sin, cos, pi, sqrt\n",
    "from random import gauss as carl_friedrich\n",
    "import random\n",
    "# Use this class as an entry to random functions.\n",
    "class Random:\n",
    "    @staticmethod\n",
    "    def gauss(mu, sigma):\n",
    "        return carl_friedrich(mu, sigma)\n",
    "\n",
    "class ParticleFilter_1:\n",
    "    def __init__(self, initial_particles,\n",
    "                 robot_width, scanner_displacement,\n",
    "                 control_motion_factor, control_turn_factor):\n",
    "        # The random generator to use.\n",
    "        self.random = Random\n",
    "\n",
    "        # The particles.\n",
    "        self.particles = initial_particles\n",
    "\n",
    "        # Some constants.\n",
    "        self.robot_width = robot_width\n",
    "        self.scanner_displacement = scanner_displacement\n",
    "        self.control_motion_factor = control_motion_factor\n",
    "        self.control_turn_factor = control_turn_factor\n",
    "\n",
    "    # State transition. This is exactly the same method as in the Kalman filter.\n",
    "    @staticmethod\n",
    "    def g(state, control, w):\n",
    "        x, y, theta = state\n",
    "        l, r = control\n",
    "        if r != l:\n",
    "            alpha = (r - l) / w\n",
    "            rad = l/alpha\n",
    "            g1 = x + (rad + w/2.)*(sin(theta+alpha) - sin(theta))\n",
    "            g2 = y + (rad + w/2.)*(-cos(theta+alpha) + cos(theta))\n",
    "            g3 = (theta + alpha + pi) % (2*pi) - pi\n",
    "        else:\n",
    "            g1 = x + l * cos(theta)\n",
    "            g2 = y + l * sin(theta)\n",
    "            g3 = theta\n",
    "\n",
    "        return (g1, g2, g3)\n",
    "\n",
    "    def predict(self, control):\n",
    "        \"\"\"The prediction step of the particle filter.\"\"\"\n",
    "        left, right = control\n",
    "\n",
    "        # Compute left and right variance.\n",
    "        # alpha_1 is self.control_motion_factor.\n",
    "        # alpha_2 is self.control_turn_factor.\n",
    "        # Then, do a loop over all self.particles and construct a new\n",
    "        # list of particles.\n",
    "        # In the end, assign the new list of particles to self.particles.\n",
    "        # NOTE: For sampling, use self.random.gauss(mu, sigma).\n",
    "        # (Note sigma in this call is the standard deviation,\n",
    "        # not the variance.)\n",
    "        # DO NOT import and use other sampling modules, as this will break\n",
    "        # the tests.\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "\n",
    "        sigma_l = (self.control_motion_factor*left)**2 + (self.control_turn_factor*(left-right))**2\n",
    "        sigma_r = (self.control_motion_factor*right)**2 + (self.control_turn_factor*(left-right))**2\n",
    "        particles_predict = []\n",
    "        \n",
    "        for i in self.particles:\n",
    "            sample_l = self.random.gauss(left, sqrt(sigma_l))\n",
    "            sample_r = self.random.gauss(right, sqrt(sigma_r))\n",
    "            control_sample = [sample_l,sample_r]\n",
    "            particles_predict.append(self.g(i, control_sample, self.robot_width))\n",
    "        \n",
    "        self.particles = particles_predict\n",
    "        \n",
    "        \n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    def print_particles(self, file_desc):\n",
    "        \"\"\"Prints particles to given file_desc output.\"\"\"\n",
    "        if not self.particles:\n",
    "            return\n",
    "        print(\"PA\", end=' ', file=file_desc)\n",
    "        for p in self.particles:\n",
    "            print(\"%.0f %.0f %.3f\" % p, end=' ', file=file_desc)\n",
    "        print(file=file_desc)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Robot constants.\n",
    "    scanner_displacement = 30.0\n",
    "    ticks_to_mm = 0.349\n",
    "    robot_width = 155.0\n",
    "\n",
    "    # Filter constants.\n",
    "    control_motion_factor = 0.35  # Error in motor control.\n",
    "    control_turn_factor = 0.6  # Additional error due to slip when turning.\n",
    "\n",
    "    # Generate initial particles. Each particle is (x, y, theta).\n",
    "    number_of_particles = 300\n",
    "    measured_state = (1850.0, 1897.0, 213.0 / 180.0 * pi)\n",
    "    standard_deviations = (100.0, 100.0, 10.0 / 180.0 * pi)\n",
    "    initial_particles = []\n",
    "    for i in range(number_of_particles):\n",
    "        initial_particles.append(tuple([\n",
    "            Random.gauss(measured_state[j], standard_deviations[j])\n",
    "            for j in range(3)]))\n",
    "\n",
    "    # Setup filter.\n",
    "    pf = ParticleFilter_1(initial_particles,\n",
    "                          robot_width, scanner_displacement,\n",
    "                          control_motion_factor, control_turn_factor)\n",
    "\n",
    "    # Read data.\n",
    "    logfile = LegoLogfile()\n",
    "    logfile.read(\"robot4_motors.txt\")\n",
    "\n",
    "    # Loop over all motor tick records.\n",
    "    # This is the particle filter loop, with prediction and correction.\n",
    "    f = open(\"particle_filter_predicted.txt\", \"w\")\n",
    "    for i in range(len(logfile.motor_ticks)):\n",
    "        # Prediction.\n",
    "        control = [x * ticks_to_mm for x in logfile.motor_ticks[i]]\n",
    "        pf.predict(control)\n",
    "\n",
    "        # Output particles.\n",
    "        pf.print_particles(f)\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3a3b041d2367053d971bd5ec8cb9846",
     "grade": false,
     "grade_id": "cell-5007b89195fe36e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### First, let's have a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceebd349fb3847708c78b2e0a5b1e9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Output(layout=Layout(width='600px')), Output(layout=Layout(width='600px')))), Ou…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute this to run the interactive viewer.\n",
    "import ipy_logfile_viewer as lfv\n",
    "v = lfv.IPYLogfileViewer(files=[\"particle_filter_predicted.txt\", \"robot4_reference.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6013ed27ef9f5299d9eb24118acd097f",
     "grade": false,
     "grade_id": "cell-c401f25a11c65426",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Here is the test for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70c2aecd72a2c8272fe11180a1a15291",
     "grade": true,
     "grade_id": "cell-69cbb5595839b2ce",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Particle filter predicion test.\n",
    "from random import uniform\n",
    "class Random_hook:\n",
    "    def __init__(self):\n",
    "        self.log = {}\n",
    "    def gauss(self, mu, sigma):\n",
    "        key = uniform(0,1)\n",
    "        self.log[key] = (mu, sigma)\n",
    "        return key\n",
    "\n",
    "def public_test(the_pf_class):\n",
    "    class TestPF(the_pf_class):\n",
    "        def __init__(self, ip, rw, sd, cmf, ctf):\n",
    "            super().__init__(ip, rw, sd, cmf, ctf)\n",
    "            self.random = Random_hook()\n",
    "            self.g = self.g_hook\n",
    "            self.log = {}\n",
    "\n",
    "        def g_hook(self, state, control, w):\n",
    "            key = uniform(0,1)\n",
    "            self.log[key] = (state, control)\n",
    "            return (key, key, key)\n",
    "\n",
    "    # Constants.\n",
    "    scanner_displacement = uniform(10, 40)\n",
    "    robot_width = uniform(100, 200)\n",
    "    control_motion_factor = uniform(0.2, 0.5)\n",
    "    control_turn_factor = uniform(0.4, 0.8)\n",
    "\n",
    "    # Draw initial particles and run one prediction step.\n",
    "    particle_number = 10\n",
    "    init_part = [(uniform(-10,10), uniform(-10,10), uniform(-3.14, 3.14))\\\n",
    "                 for _ in range(particle_number)]\n",
    "    pf = TestPF(init_part[:],\n",
    "                robot_width, scanner_displacement,\n",
    "                control_motion_factor, control_turn_factor)\n",
    "    l, r = uniform(-1,1), uniform(2,3)\n",
    "    pf.predict((l, r))\n",
    "    \n",
    "    # Tests. Checks chain from end to beginning.\n",
    "    # Must have correct number of output particles.\n",
    "    if len(pf.particles) != particle_number:\n",
    "        print(\"Generated wrong number of particles.\")\n",
    "        return False\n",
    "    # All paticles must be distinct.\n",
    "    if len(set(pf.particles)) != particle_number:\n",
    "        print(\"Generated identical particles.\")\n",
    "        return False\n",
    "    # For each particle, get state and control.\n",
    "    g_in = [pf.log[s[0]] for s in pf.particles]\n",
    "    # The particle states must be exactly the initial particles.\n",
    "    if set(init_part) != set(s for s,c in g_in):\n",
    "        print(\"Called g() with wrong particle states.\")\n",
    "        return False\n",
    "    # Now follow (l,r). All must have been different random calls.\n",
    "    ctrl = [c for s,c in g_in]\n",
    "    ctrl_flat = set(c for pair in ctrl for c in pair)\n",
    "    if len(ctrl_flat) != 2*particle_number:\n",
    "        print(\"Seems you did not randomize each particle.\")\n",
    "        return False\n",
    "    # All used random calls must have the same (l, sigma_l) or (r, sigma_r).\n",
    "    lefts  = set(pf.random.log[c[0]] for c in ctrl)\n",
    "    rights = set(pf.random.log[c[1]] for c in ctrl)\n",
    "    if len(lefts) != 1 or len(rights) != 1:\n",
    "        print(\"Called self.random.gauss with varying parameter values?\")\n",
    "        return False\n",
    "    # Finally, check if stddev are correct.\n",
    "    lval, sl, rval, sr = lefts.pop() + rights.pop()\n",
    "    l2, r2, sl2, sr2 = l*l, r*r, sl*sl, sr*sr\n",
    "    a12, a22 = control_motion_factor**2, control_turn_factor**2\n",
    "    if lval != l or rval != r or\\\n",
    "       abs(sl2-sr2 - a12*(l2-r2)) > 1e-5 or\\\n",
    "       abs(sl2+sr2 - (a12+2*a22)*(l2+r2)+4*a22*l*r) > 1e-5:\n",
    "        print(\"Called self.random.gauss with wrong parameter values.\")\n",
    "        return False\n",
    "    return True\n",
    "assert public_test(ParticleFilter_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0475d577aebfe442dd7b6f0e1d3cfad5",
     "grade": false,
     "grade_id": "cell-d87f346a8748bc57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introducing the particle filter correction step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd85bd8b9d687a013390b2b6be6d9f18",
     "grade": false,
     "grade_id": "cell-a605344e533f8504",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"//av.tib.eu/player/49021\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc0a814c850>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't see a video below, run this cell.\n",
    "from IPython.display import IFrame\n",
    "IFrame(\"https://www.youtube.com/embed/kZhOJgooMxI\" if \"YouTube\" in globals() else \"//av.tib.eu/player/49021\",\n",
    "       width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7e9482e26c192dac49dfdae661385a4",
     "grade": false,
     "grade_id": "cell-57b81c14dc07a184",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Programming assignment: program the correction step, first part: `probability_of_measurement` (10 Points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb87979d8e8f421e618379da4a418022",
     "grade": false,
     "grade_id": "cell-a6fe8de533594fe8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The particle filter, prediciton and correction.\n",
    "#\n",
    "# slam_08_b_particle_correction.\n",
    "from math import pi\n",
    "from scipy.stats import norm as normal_dist\n",
    "\n",
    "class ParticleFilter_2(ParticleFilter_1):\n",
    "    def __init__(self, initial_particles,\n",
    "                 robot_width, scanner_displacement,\n",
    "                 control_motion_factor, control_turn_factor,\n",
    "                 measurement_distance_stddev, measurement_angle_stddev):\n",
    "        super().__init__(initial_particles, robot_width, scanner_displacement,\n",
    "                         control_motion_factor, control_turn_factor)\n",
    "        self.measurement_distance_stddev = measurement_distance_stddev\n",
    "        self.measurement_angle_stddev = measurement_angle_stddev\n",
    "\n",
    "    def probability_of_measurement(self, measurement, predicted_measurement):\n",
    "        \"\"\"Given a measurement and a predicted measurement, computes\n",
    "           probability.\"\"\"\n",
    "        # Compute differences to real measurements.\n",
    "\n",
    "        # --->>> Compute difference in distance and bearing angle.\n",
    "        # Important: make sure the angle difference works correctly and does\n",
    "        # not return values offset by 2 pi or - 2 pi.\n",
    "        # You may use the following Gaussian PDF function:\n",
    "        #   normal_dist.pdf(x, mu, sigma).\n",
    "        # Note that the two parameters sigma_d and sigma_alpha discussed\n",
    "        # in the lecture are self.measurement_distance_stddev and\n",
    "        # self.measurement_angle_stddev.\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        dist,alpha = measurement\n",
    "        dist_pre, alpha_pre = predicted_measurement\n",
    "        \n",
    "        dist_diff = dist - dist_pre\n",
    "        alpha_diff = (alpha - alpha_pre + pi) % (2*pi) - pi\n",
    "            \n",
    "        alpha_prob = normal_dist.pdf(alpha_diff, 0, self.measurement_angle_stddev)\n",
    "        dist_prob = normal_dist.pdf(dist_diff, 0, self.measurement_distance_stddev)\n",
    "            \n",
    "        return alpha_prob * dist_prob\n",
    "        \n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbc6dd8dacefaf0fe137356f7e91629a",
     "grade": true,
     "grade_id": "cell-6a5321e3709c0436",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# First test: function probability_of_measurement().\n",
    "from random import uniform\n",
    "from scipy.stats import norm as normal_dist\n",
    "from math import pi\n",
    "\n",
    "def test_probability_of_measurement(the_pf_class):\n",
    "\n",
    "    d_std, a_std = 150.0, 20.0 / 180.0 * pi\n",
    "\n",
    "    # Setup filter.\n",
    "    pf = the_pf_class([], 140, 40, 0.4, 0.7, d_std, a_std)\n",
    "\n",
    "    # Test this for the full circle.\n",
    "    steps = 50\n",
    "    bad_steps = 0\n",
    "    for i in range(0,steps+1):\n",
    "        d1, dd = uniform(d_std, 1000), uniform(-d_std,d_std)\n",
    "        a1, ad = (i*2*pi/steps-pi), uniform(-a_std,a_std)\n",
    "        p_ref = normal_dist.pdf(dd, 0, d_std) * normal_dist.pdf(2*ad, 0, a_std)\n",
    "        p = pf.probability_of_measurement(\n",
    "            (d1, (a1-ad+pi)%(2*pi)-pi), (d1+dd, (a1+ad+pi)%(2*pi)-pi))\n",
    "        if abs(p-p_ref) > 1e-10:\n",
    "            bad_steps += 1\n",
    "    if bad_steps > steps/2:\n",
    "        print(\"Wrong implementation in probability_of_measurement().\")\n",
    "        return False\n",
    "    elif bad_steps > 0:\n",
    "        print(\"Check handling of angle differences in \"\\\n",
    "              \"probability_of_measurement().\")\n",
    "        return False\n",
    "    return True\n",
    "assert(test_probability_of_measurement(ParticleFilter_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4690eb9cb4ce8a6f3ec489da46359ad0",
     "grade": false,
     "grade_id": "cell-9d815a7f86fc55e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Programming assignment: program the correction step, second part: `compute_weights` (10 Points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de2e6c9e0cc7795673234fdd8998f037",
     "grade": false,
     "grade_id": "cell-c37cf98f9aaf25a2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The particle filter, prediciton and correction.\n",
    "#\n",
    "# slam_08_b_particle_correction.\n",
    "from slam_e_library import assign_cylinders\n",
    "from math import atan2, pi\n",
    "\n",
    "class ParticleFilter_3(ParticleFilter_2):\n",
    "    # Measurement. This is exactly the same method as in the Kalman filter.\n",
    "    @staticmethod\n",
    "    def h(state, landmark, scanner_displacement):\n",
    "        \"\"\"Takes a (x, y, theta) state and a (x, y) landmark, and returns the\n",
    "           corresponding (range, bearing).\"\"\"\n",
    "        dx = landmark[0] - (state[0] + scanner_displacement * cos(state[2]))\n",
    "        dy = landmark[1] - (state[1] + scanner_displacement * sin(state[2]))\n",
    "        r = sqrt(dx * dx + dy * dy)\n",
    "        alpha = (atan2(dy, dx) - state[2] + pi) % (2*pi) - pi\n",
    "        return (r, alpha)\n",
    "\n",
    "    def compute_weights(self, cylinders, landmarks):\n",
    "        \"\"\"Computes one weight for each particle, return list of weights.\"\"\"\n",
    "        weights = []\n",
    "        for p in self.particles:\n",
    "            # Get list of tuples:\n",
    "            # [ ((range_0, bearing_0), (landmark_x, landmark_y)), ... ]\n",
    "            assignment = assign_cylinders(cylinders, p,\n",
    "                self.scanner_displacement, landmarks)\n",
    "            # --->>> Insert code to compute weight for particle p here.\n",
    "            # This will require a loop over all (measurement, landmark)\n",
    "            # in assignment. Append weight to the list of weights using\n",
    "            # weights.append(...).\n",
    "            # YOUR CODE HERE\n",
    "            \n",
    "            weight = 1.0\n",
    "            \n",
    "            for pair in assignment:\n",
    "                measurement, landmark = pair\n",
    "                predicted_measurement = self.h(p, landmark, self.scanner_displacement)\n",
    "                weight *= self.probability_of_measurement(measurement, predicted_measurement)\n",
    "            \n",
    "            weights.append(weight)  \n",
    "\n",
    "            #raise NotImplementedError()\n",
    "            \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16b8a2e3a25445aac0d6948ffa276c25",
     "grade": true,
     "grade_id": "cell-28f2db3491a2bad6",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Second test: function compute_weights().\n",
    "from random import uniform as uf\n",
    "from random import randrange\n",
    "from math import pi, sin, cos, isclose\n",
    "from operator import mul\n",
    "from functools import reduce\n",
    "from slam_e_library import assign_cylinders as ass\n",
    "# Given state and landmark, get (x,y) in scanner coord system.\n",
    "def euc(state, xy, sd):\n",
    "    x,y,ct,st = state[0], state[1], cos(state[2]), sin(state[2])\n",
    "    xv, yv = xy[0]-x-sd*ct, xy[1]-y-sd*st\n",
    "    return (xv*ct+yv*st, yv*ct-xv*st)\n",
    "def test_compute_weights(the_pf_class):\n",
    "    num_experiments = 10\n",
    "    num_particles = 5\n",
    "    num_landmarks = 3\n",
    "    for _ in range(num_experiments):\n",
    "        part = [(uf(0,200),uf(0,200),uf(-pi,pi)) for _ in range(num_particles)]\n",
    "        lm = [(uf(0,200),uf(0,200)) for _ in range(num_landmarks)]\n",
    "        pf = the_pf_class(part[:], 140, 40, 0.3, 0.5, 180, 0.25)\n",
    "        p0 = part[randrange(num_particles)]\n",
    "        obs = [pf.h(p0,l,40)+euc(p0,l,40) for l in lm]\n",
    "        w = pf.compute_weights(obs,lm)\n",
    "        if len(w) != num_particles:\n",
    "            print(\"Error: compute_weights must produce one weight\"\\\n",
    "                  \"per particle.\")\n",
    "            return False\n",
    "        if not all(isclose(a,b) for a,b in zip(w,(reduce(mul,(\n",
    "           pf.probability_of_measurement(m,pf.h(p,l,40))\n",
    "           for m,l in ass(obs,p,40,lm))) for p in part))):\n",
    "            print(\"Error: compte_weights() produces wrong weights.\")\n",
    "            return False\n",
    "    return True\n",
    "assert(test_compute_weights(ParticleFilter_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d311bd0b6e22ad730f52283f62540cf6",
     "grade": false,
     "grade_id": "cell-22fbab4e5522a1ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Programming assignment: program the correction step, third part: `resample` (10 Points).\n",
    "\n",
    "**Please note this hint:** In the video, `offset += random.uniform(0, 2*max_weight)` is proposed to move the pointer forward. While this works in general, it introduces a small error. Instead, use `offset += random.uniform(0, sum_weight)`, where `sum_weight` is the sum of all weights. (Note that Python has a built-in function `sum` which can be used to compute the sum of an iterable (e.g., a list).) If you are interested in this topic, you will find more under the keyword *low variance sampling*, e.g. in the Probabilistic Robotics Book of Thrun, Burgard and Fox (Table 4.4 in the book). If you like, you can also implement the program listed there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a2cc3a4f26b3c5be96b097be563a176",
     "grade": false,
     "grade_id": "cell-1dde53866e3a825d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The particle filter, prediciton and correction.\n",
    "#\n",
    "# slam_08_b_particle_correction.\n",
    "import random\n",
    "\n",
    "class ParticleFilter_4(ParticleFilter_3):\n",
    "\n",
    "    def resample(self, weights):\n",
    "        \"\"\"Return a list of particles which have been resampled, proportional\n",
    "           to the given weights.\"\"\"\n",
    "        # You may implement the 'resampling wheel' algorithm\n",
    "        # described in the lecture.\n",
    "        # HINT: In the video, offset += random.uniform(0, 2*max_weight) is proposed.\n",
    "        # Instead, use offset += random.uniform(0, sum_weight), where sum_weight is\n",
    "        # the sum of all weights.\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        new_particles = []\n",
    "        max_weight = sum(weights) # max of weights\n",
    "        Length_weights = len(weights) # length of weights\n",
    "        offset = 0.0\n",
    "        index = random.randint(0,Length_weights-1)\n",
    "        \n",
    "        for i in range(Length_weights):\n",
    "            offset += random.uniform(0, 2*max_weight)\n",
    "            while (offset > weights[index]):\n",
    "                offset -= weights[index]\n",
    "                index = (index+1) % Length_weights\n",
    "            new_particles.append(self.particles[index])\n",
    "            \n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        return new_particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6867ced545ddb62ea4c473a1228db0f0",
     "grade": true,
     "grade_id": "cell-6162f3d17673ff35",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Third test: function resample().\n",
    "import numpy as np\n",
    "def test_resample(the_pf_class):\n",
    "    # Setup filter.\n",
    "    num_particles = 5\n",
    "    experiment_loops = 4\n",
    "    outer_loops = 50\n",
    "    inner_loops = 10000\n",
    "    pf = the_pf_class([(i,i,i) for i in range(num_particles)],\n",
    "                      140, 40, 0.3, 0.5, 180, 0.25)\n",
    "    # Experiment is OK if it succeeds once.\n",
    "    for e in range(experiment_loops):\n",
    "        # Make up some weights. Force them to be a bit different by\n",
    "        # adding arange, shuffle to prevent always increasing values.\n",
    "        weights = np.random.uniform(size=num_particles)+np.arange(num_particles)\n",
    "        np.random.shuffle(weights)\n",
    "        weights /= weights.sum()\n",
    "        histo = np.zeros(num_particles)  # Histogram to collect choices.\n",
    "        for o in range(1, outer_loops):\n",
    "            for i in range(inner_loops):\n",
    "                np.add.at(histo, np.array(pf.resample(weights))[:,0], 1)\n",
    "            rel_freq = histo / (num_particles*inner_loops*o)\n",
    "            # You may uncomment this to check how the error develops.\n",
    "            #print(np.max(np.abs(rel_freq-weights)))\n",
    "            if np.max(np.abs(rel_freq-weights)) < 1e-3:\n",
    "                return True\n",
    "    print(\"It seems your draws do not implement the correct probabilities.\")\n",
    "    return False\n",
    "assert(test_resample(ParticleFilter_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3686967f9ba322eeb66260244a5468e",
     "grade": false,
     "grade_id": "cell-93ae037f6dd0be05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Not, we put everything together!\n",
    "Running this will generate the file `particle_filter_corrected.txt` which we will look at in the next but one cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this to generate particle_filter_corrected.txt.\n",
    "# The particle filter, prediciton and correction.\n",
    "#\n",
    "# slam_08_b_particle_correction.\n",
    "from lego_robot import *\n",
    "from slam_e_library import get_cylinders_from_scan\n",
    "from math import pi\n",
    "import random\n",
    "\n",
    "class ParticleFilter_5(ParticleFilter_4):\n",
    "    \n",
    "    # Note there is no need to copy & paste your functions above to here, since\n",
    "    # they are inherited.\n",
    "\n",
    "    def correct(self, cylinders, landmarks):\n",
    "        \"\"\"The correction step of the particle filter.\"\"\"\n",
    "        # First compute all weights.\n",
    "        weights = self.compute_weights(cylinders, landmarks)\n",
    "        # Then resample, based on the weight array.\n",
    "        self.particles = self.resample(weights)\n",
    "\n",
    "    def print_particles(self, file_desc):\n",
    "        \"\"\"Prints particles to given file_desc output.\"\"\"\n",
    "        if not self.particles:\n",
    "            return\n",
    "        print(\"PA\", end=' ', file=file_desc)\n",
    "        for p in self.particles:\n",
    "            print(\"%.0f %.0f %.3f\" % p, end=' ', file=file_desc)\n",
    "        print(file=file_desc)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Robot constants.\n",
    "    scanner_displacement = 30.0\n",
    "    ticks_to_mm = 0.349\n",
    "    robot_width = 155.0\n",
    "\n",
    "    # Cylinder extraction and matching constants.\n",
    "    minimum_valid_distance = 20.0\n",
    "    depth_jump = 100.0\n",
    "    cylinder_offset = 90.0\n",
    "\n",
    "    # Filter constants.\n",
    "    control_motion_factor = 0.35  # Error in motor control.\n",
    "    control_turn_factor = 0.6  # Additional error due to slip when turning.\n",
    "    measurement_distance_stddev = 200.0  # Distance measurement error of cylinders.\n",
    "    measurement_angle_stddev = 15.0 / 180.0 * pi  # Angle measurement error.\n",
    "\n",
    "    # Generate initial particles. Each particle is (x, y, theta).\n",
    "    number_of_particles = 50\n",
    "    measured_state = (1850.0, 1897.0, 213.0 / 180.0 * pi)\n",
    "    standard_deviations = (100.0, 100.0, 10.0 / 180.0 * pi)\n",
    "    initial_particles = []\n",
    "    for i in range(number_of_particles):\n",
    "        initial_particles.append(tuple([\n",
    "            random.gauss(measured_state[j], standard_deviations[j])\n",
    "            for j in range(3)]))\n",
    "\n",
    "    # Setup filter.\n",
    "    pf = ParticleFilter_5(initial_particles,\n",
    "                          robot_width, scanner_displacement,\n",
    "                          control_motion_factor, control_turn_factor,\n",
    "                          measurement_distance_stddev,\n",
    "                          measurement_angle_stddev)\n",
    "\n",
    "    # Read data.\n",
    "    logfile = LegoLogfile()\n",
    "    logfile.read(\"robot4_motors.txt\")\n",
    "    logfile.read(\"robot4_scan.txt\")\n",
    "    logfile.read(\"robot_arena_landmarks.txt\")\n",
    "    reference_cylinders = [l[1:3] for l in logfile.landmarks]\n",
    "\n",
    "    # Loop over all motor tick records.\n",
    "    # This is the particle filter loop, with prediction and correction.\n",
    "    f = open(\"particle_filter_corrected.txt\", \"w\")\n",
    "    for i in range(len(logfile.motor_ticks)):\n",
    "        # Prediction.\n",
    "        control = [x * ticks_to_mm for x in logfile.motor_ticks[i]]\n",
    "        pf.predict(control)\n",
    "\n",
    "        # Correction.\n",
    "        cylinders = get_cylinders_from_scan(logfile.scan_data[i], depth_jump,\n",
    "            minimum_valid_distance, cylinder_offset)\n",
    "        pf.correct(cylinders, reference_cylinders)\n",
    "\n",
    "        # Output particles.\n",
    "        pf.print_particles(f)\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84e7143d6220660de18c39d2da3f50c7",
     "grade": false,
     "grade_id": "cell-fd04e8d52d8db6ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's have a look in the logfile viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5810d51579bf4814a076c847bcf7f97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Output(layout=Layout(width='600px')), Output(layout=Layout(width='600px')))), Ou…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute this to run the interactive viewer.\n",
    "import ipy_logfile_viewer as lfv\n",
    "v = lfv.IPYLogfileViewer(files=[\"particle_filter_corrected.txt\", \"robot4_reference.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "206d1fe52f529d4abb0884fd6a71e797",
     "grade": false,
     "grade_id": "cell-afbe4de1d2481527",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Density estimation\n",
    "Now, we have a lot of particles, but where is the robot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fadc4cd0218379a4e7a71e124da773b2",
     "grade": false,
     "grade_id": "cell-7d83a95cd204d487",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"//av.tib.eu/player/49022\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc09764efd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't see a video below, run this cell.\n",
    "from IPython.display import IFrame\n",
    "IFrame(\"https://www.youtube.com/embed/x-bHZMo1a28\" if \"YouTube\" in globals() else \"//av.tib.eu/player/49022\",\n",
    "       width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a7f446d489dae5ffd5998316aa08d9c",
     "grade": false,
     "grade_id": "cell-cdd0a9df2a97b635",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Programming assignment: compute the mean state (10 Points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "775aa32fe813e37fecae0e54a7315df4",
     "grade": false,
     "grade_id": "cell-de40729ced6dea5d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ParticleFilter_6(ParticleFilter_5):\n",
    "\n",
    "    def get_mean(self):\n",
    "        \"\"\"Compute mean position and heading from all particles.\"\"\"\n",
    "        # Return a tuple: (mean_x, mean_y, mean_heading).\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        sum_x = 0.0\n",
    "        sum_y = 0.0\n",
    "        sum_vx = 0.0\n",
    "        sum_vy = 0.0 \n",
    "        Length_particles = len(self.particles)\n",
    "        \n",
    "        for p in self.particles:\n",
    "            sum_x += p[0]\n",
    "            sum_y += p[1]\n",
    "            sum_vx += cos(p[2])\n",
    "            sum_vy += sin(p[2])\n",
    "            \n",
    "        mean_x = sum_x/Length_particles\n",
    "        mean_y = sum_y/Length_particles\n",
    "        mean_heading = atan2(sum_vy, sum_vx)\n",
    "        \n",
    "        return (mean_x, mean_y, mean_heading) \n",
    "        #raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e025038105b053bb014dc4363caeea8b",
     "grade": true,
     "grade_id": "cell-ba6d754ca5e845da",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test the get_mean function.\n",
    "from random import randrange\n",
    "import numpy as np\n",
    "def test_mean(the_pf_class):\n",
    "    n = 20 + 2*randrange(10)  # Random number of particles.\n",
    "    xyt = np.random.uniform(-1,1,size=(n,3))\n",
    "    xyt[n//2:n,:] = -xyt[0:n//2,:]\n",
    "    xytc = np.random.uniform(-5,5,3)  # Random center.\n",
    "    xytc[2] = np.random.uniform(-0.1, 0.1)  # Heading.\n",
    "    xyts = xyt + xytc  # Shift.\n",
    "    # Run through get_mean and test.\n",
    "    pf = the_pf_class([tuple(p) for p in xyts], 140, 40, 0.3, 0.5, 180, 0.25)\n",
    "    c = pf.get_mean()\n",
    "    if (abs(c-xytc) > 1e-10).any():\n",
    "        print(\"Your get_mean() produces the wrong result.\")\n",
    "        return False\n",
    "    # Do the same once more for dangerous heading angles.\n",
    "    xytc = np.random.uniform(-5,5,3)\n",
    "    xytc[2] = np.random.uniform(-0.1, 0.1) - np.pi\n",
    "    xyts = xyt + xytc\n",
    "    xyts[:,2] = (xyts[:,2]+np.pi)%(2*np.pi)-np.pi  # Normalize -pi..pi range.\n",
    "    pf = the_pf_class([tuple(p) for p in xyts], 140, 40, 0.3, 0.5, 180, 0.25)\n",
    "    c = np.array(pf.get_mean())\n",
    "    # Normalize angle for reference and result.\n",
    "    xytc[2] = (xytc[2]+np.pi)%(2*np.pi)-np.pi\n",
    "    c[2] = (c[2]+np.pi)%(2*np.pi)-np.pi\n",
    "    if (abs(c-xytc) > 1e-10).any():\n",
    "        print(\"Wrong mean - check your computation of the heading.\")\n",
    "        return False\n",
    "    return True\n",
    "assert(test_mean(ParticleFilter_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f46ce18ea546cf076e88bdb2d8061d3",
     "grade": false,
     "grade_id": "cell-347272efc05cf3fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Run the following cell to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The particle filter, prediciton and correction.\n",
    "# In addition to the filtered particles, the density is estimated. In this\n",
    "# simple case, the mean position and heading is computed from all particles.\n",
    "#\n",
    "# slam_08_c_density_estimation.\n",
    "from lego_robot import *\n",
    "from slam_e_library import get_cylinders_from_scan\n",
    "from math import sin, cos, pi\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Robot constants.\n",
    "    scanner_displacement = 30.0\n",
    "    ticks_to_mm = 0.349\n",
    "    robot_width = 155.0\n",
    "\n",
    "    # Cylinder extraction and matching constants.\n",
    "    minimum_valid_distance = 20.0\n",
    "    depth_jump = 100.0\n",
    "    cylinder_offset = 90.0\n",
    "\n",
    "    # Filter constants.\n",
    "    control_motion_factor = 0.35  # Error in motor control.\n",
    "    control_turn_factor = 0.6  # Additional error due to slip when turning.\n",
    "    measurement_distance_stddev = 200.0  # Distance measurement error of cylinders.\n",
    "    measurement_angle_stddev = 15.0 / 180.0 * pi  # Angle measurement error.\n",
    "\n",
    "    # Generate initial particles. Each particle is (x, y, theta).\n",
    "    number_of_particles = 50\n",
    "    measured_state = (1850.0, 1897.0, 213.0 / 180.0 * pi)\n",
    "    standard_deviations = (100.0, 100.0, 10.0 / 180.0 * pi)\n",
    "    initial_particles = []\n",
    "    for i in range(number_of_particles):\n",
    "        initial_particles.append(tuple([\n",
    "            random.gauss(measured_state[j], standard_deviations[j])\n",
    "            for j in range(3)]))\n",
    "\n",
    "    # Setup filter.\n",
    "    pf = ParticleFilter_6(initial_particles,\n",
    "                          robot_width, scanner_displacement,\n",
    "                          control_motion_factor, control_turn_factor,\n",
    "                          measurement_distance_stddev,\n",
    "                          measurement_angle_stddev)\n",
    "\n",
    "    # Read data.\n",
    "    logfile = LegoLogfile()\n",
    "    logfile.read(\"robot4_motors.txt\")\n",
    "    logfile.read(\"robot4_scan.txt\")\n",
    "    logfile.read(\"robot_arena_landmarks.txt\")\n",
    "    reference_cylinders = [l[1:3] for l in logfile.landmarks]\n",
    "\n",
    "    # Loop over all motor tick records.\n",
    "    # This is the particle filter loop, with prediction and correction.\n",
    "    f = open(\"particle_filter_mean.txt\", \"w\")\n",
    "    for i in range(len(logfile.motor_ticks)):\n",
    "        # Prediction.\n",
    "        control = [x * ticks_to_mm for x in logfile.motor_ticks[i]]\n",
    "        pf.predict(control)\n",
    "\n",
    "        # Correction.\n",
    "        cylinders = get_cylinders_from_scan(logfile.scan_data[i], depth_jump,\n",
    "            minimum_valid_distance, cylinder_offset)\n",
    "        pf.correct(cylinders, reference_cylinders)\n",
    "\n",
    "        # Output particles.\n",
    "        pf.print_particles(f)\n",
    "        \n",
    "        # Output state estimated from all particles.\n",
    "        mean = pf.get_mean()\n",
    "        print(\"F %.0f %.0f %.3f\" %\\\n",
    "              (mean[0] + scanner_displacement * cos(mean[2]),\n",
    "               mean[1] + scanner_displacement * sin(mean[2]),\n",
    "               mean[2]), file=f)\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b574863f39c3171251cf05caaab6c160",
     "grade": false,
     "grade_id": "cell-25bc66ea0a718dfd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check the output using the logfile viewer.\n",
    "Since the particle filter uses random draws in the prediction and correction steps, you may re-run the cell above to get a different result! Usually, it will have bumps in different locations. The result is discussed in the next video below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768b7b63adf148428eea0384c689c32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Output(layout=Layout(width='600px')), Output(layout=Layout(width='600px')))), Ou…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute this to run the interactive viewer.\n",
    "import ipy_logfile_viewer as lfv\n",
    "v = lfv.IPYLogfileViewer(files=[\"particle_filter_mean.txt\", \"robot4_reference.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae9c67cd69bf839c7c34d5715ea16db1",
     "grade": false,
     "grade_id": "cell-e515b8924657b957",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Resampling, initialization, and error ellipses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7757f976805d208d2c3fab438c5adc26",
     "grade": false,
     "grade_id": "cell-fffd83a37d2590fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"//av.tib.eu/player/49023\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc09728da10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't see a video below, run this cell.\n",
    "from IPython.display import IFrame\n",
    "IFrame(\"https://www.youtube.com/embed/Il2xzNchp1M\" if \"YouTube\" in globals() else \"//av.tib.eu/player/49023\",\n",
    "       width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d601bda9d1c253ab4a2120bdc06e6949",
     "grade": false,
     "grade_id": "cell-738179fbe05cabf0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Run this code to produce the error ellipses.\n",
    "Note that if you use a large number of particles, this may take a while (in a Jupyter Notebook, the *kernel status indicator* will stay *busy* for a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The particle filter, prediciton and correction.\n",
    "# In addition to the previous code:\n",
    "# 1.\n",
    "# the second moments are computed and are output as an error ellipse and\n",
    "# heading variance.\n",
    "# 2.\n",
    "# the particles are initialized uniformly distributed in the arena, and a\n",
    "# larger number of particles is used.\n",
    "# 3.\n",
    "# predict and correct are only called when control is nonzero.\n",
    "#\n",
    "# slam_08_d_density_error_ellipse.\n",
    "from lego_robot import *\n",
    "from math import sin, cos, pi\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class ParticleFilter_7(ParticleFilter_6):\n",
    "    # *** Modification 1: Extension: This computes the error ellipse.\n",
    "    def get_error_ellipse_and_heading_variance(self, mean):\n",
    "        \"\"\"Returns a tuple: (angle, stddev1, stddev2, heading-stddev) which is\n",
    "           the orientation of the xy error ellipse, the half axis 1, half axis 2,\n",
    "           and the standard deviation of the heading.\"\"\"\n",
    "        center_x, center_y, center_heading = mean\n",
    "        n = len(self.particles)\n",
    "        if n < 2:\n",
    "            return (0.0, 0.0, 0.0, 0.0)\n",
    "\n",
    "        # Compute covariance matrix in xy.\n",
    "        sxx, sxy, syy = 0.0, 0.0, 0.0\n",
    "        for p in self.particles:\n",
    "            dx = p[0] - center_x\n",
    "            dy = p[1] - center_y\n",
    "            sxx += dx * dx\n",
    "            sxy += dx * dy\n",
    "            syy += dy * dy\n",
    "        cov_xy = np.array([[sxx, sxy], [sxy, syy]]) / (n-1)\n",
    "\n",
    "        # Get variance of heading.\n",
    "        var_heading = 0.0\n",
    "        for p in self.particles:\n",
    "            dh = (p[2] - center_heading + pi) % (2*pi) - pi\n",
    "            var_heading += dh * dh\n",
    "        var_heading = var_heading / (n-1)\n",
    "\n",
    "        # Convert xy to error ellipse.\n",
    "        eigenvals, eigenvects = np.linalg.eig(cov_xy)\n",
    "        ellipse_angle = atan2(eigenvects[1,0], eigenvects[0,0])\n",
    "\n",
    "        return (ellipse_angle, sqrt(abs(eigenvals[0])),\n",
    "                sqrt(abs(eigenvals[1])),\n",
    "                sqrt(var_heading))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Robot constants.\n",
    "    scanner_displacement = 30.0\n",
    "    ticks_to_mm = 0.349\n",
    "    robot_width = 155.0\n",
    "\n",
    "    # Cylinder extraction and matching constants.\n",
    "    minimum_valid_distance = 20.0\n",
    "    depth_jump = 100.0\n",
    "    cylinder_offset = 90.0\n",
    "\n",
    "    # Filter constants.\n",
    "    control_motion_factor = 0.35  # Error in motor control.\n",
    "    control_turn_factor = 0.6  # Additional error due to slip when turning.\n",
    "    measurement_distance_stddev = 200.0  # Distance measurement error of cylinders.\n",
    "    measurement_angle_stddev = 15.0 / 180.0 * pi  # Angle measurement error.\n",
    "\n",
    "    # Generate initial particles. Each particle is (x, y, theta).\n",
    "    # *** Modification 2: Generate the particles uniformly distributed.\n",
    "    # *** Also, use a large number of particles.\n",
    "    # Change the following value \"30\" to something larger, e.g. 300 or 500.\n",
    "    # NOTE: as the execution then takes a while, set it back to 30 before you \"validate\"\n",
    "    # or \"submit\".\n",
    "    number_of_particles = 30 # Set this larger. Set smaller before submission.\n",
    "    # Alternative: uniform init.\n",
    "    initial_particles = []\n",
    "    for i in range(number_of_particles):\n",
    "        initial_particles.append((\n",
    "            random.uniform(0.0, 2000.0), random.uniform(0.0, 2000.0),\n",
    "            random.uniform(-pi, pi)))\n",
    "\n",
    "    # Setup filter.\n",
    "    pf = ParticleFilter_7(initial_particles,\n",
    "                          robot_width, scanner_displacement,\n",
    "                          control_motion_factor, control_turn_factor,\n",
    "                          measurement_distance_stddev,\n",
    "                          measurement_angle_stddev)\n",
    "\n",
    "    # Read data.\n",
    "    logfile = LegoLogfile()\n",
    "    logfile.read(\"robot4_motors.txt\")\n",
    "    logfile.read(\"robot4_scan.txt\")\n",
    "    logfile.read(\"robot_arena_landmarks.txt\")\n",
    "    reference_cylinders = [l[1:3] for l in logfile.landmarks]\n",
    "\n",
    "    # Loop over all motor tick records.\n",
    "    # This is the particle filter loop, with prediction and correction.\n",
    "    f = open(\"particle_filter_ellipse.txt\", \"w\")\n",
    "    for i in range(len(logfile.motor_ticks)):\n",
    "        control = [x * ticks_to_mm for x in logfile.motor_ticks[i]]\n",
    "        # *** Modification 3: Call the predict/correct step only if there\n",
    "        # *** is nonzero control.\n",
    "        if control != [0.0, 0.0]:\n",
    "            # Prediction.\n",
    "            pf.predict(control)\n",
    "\n",
    "            # Correction.\n",
    "            cylinders = get_cylinders_from_scan(logfile.scan_data[i], depth_jump,\n",
    "                minimum_valid_distance, cylinder_offset)\n",
    "            pf.correct(cylinders, reference_cylinders)\n",
    "\n",
    "        # Output particles.\n",
    "        pf.print_particles(f)\n",
    "        \n",
    "        # Output state estimated from all particles.\n",
    "        mean = pf.get_mean()\n",
    "        print(\"F %.0f %.0f %.3f\" %\\\n",
    "              (mean[0] + scanner_displacement * cos(mean[2]),\n",
    "               mean[1] + scanner_displacement * sin(mean[2]),\n",
    "               mean[2]), file=f)\n",
    "\n",
    "        # Output error ellipse and standard deviation of heading.\n",
    "        errors = pf.get_error_ellipse_and_heading_variance(mean)\n",
    "        print(\"E %.3f %.0f %.0f %.3f\" % errors, file=f)\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32a1b9c93a3686ea01e063a84a3ff796",
     "grade": false,
     "grade_id": "cell-34e88b0625cfcd26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check the results using the logfile viewer.\n",
    "You may occasionally see that the filter is not able to catch up with the correct position, and the resulting trajectory is \"complete nonsense\". If you encounter this, re-run the previous cell. If it persists, increase `number_of_particles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1819decec7b34c03971833fe7cec9197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Output(layout=Layout(width='600px')), Output(layout=Layout(width='600px')))), Ou…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute this to run the interactive viewer.\n",
    "import ipy_logfile_viewer as lfv\n",
    "v = lfv.IPYLogfileViewer(files=[\"particle_filter_ellipse.txt\", \"robot4_reference.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a51013c993a8ff22c17f6aad2c3c8190",
     "grade": false,
     "grade_id": "cell-707b60908e7914eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Here is one last question:\n",
    "In the *variance of weights* approach discussed in the previous video, when should we resample?\n",
    "- If the variance of weights is small  *or*\n",
    "- If the variance of weights is large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e90c487245edbb7e8eb07fdca98e94b2",
     "grade": false,
     "grade_id": "cell-7a618fe6e62dac7a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We will have to resample if the variance is... answer with either \"small\" or \"large\".\n",
    "if_the_variance_is = \"large\"\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25fa180a8139bfb892ebdbc63ebbada1",
     "grade": true,
     "grade_id": "cell-59f5342f6addf4dd",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from hashlib import shake_128\n",
    "def public_string_test(the_answer_string, reference):\n",
    "    m = shake_128()\n",
    "    m.update(the_answer_string.encode())\n",
    "    return m.hexdigest(4) == reference\n",
    "assert public_string_test(\"variance=\"+if_the_variance_is.strip().lower(), '45eeff02'), \"Oh no, your answer is wrong!\""
   ]
  }
 ],
 "metadata": {
  "copyright": "(c) Claus Brenner 2020",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
