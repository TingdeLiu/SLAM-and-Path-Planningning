{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aeb06a465638c37f281beb32e9989038",
     "grade": false,
     "grade_id": "cell-4b3cb5444846bd01",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# SLAM Unit G\n",
    "This unit is about the particle filter SLAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1c46363e587313670a7a571b5171eb8",
     "grade": false,
     "grade_id": "cell-17edd3f047278dcc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Rao-Blackwellized particle filter\n",
    "We first recap a few of our earlier results: the particle filter (for localization only) and the extended Kalman filter (EKF) SLAM (for localization *and* mapping). Starting from there, we think about how a particle filter SLAM would look like and introduce a special way to realize it, named the Rao-Blackwellized particle filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"//av.tib.eu/player/49032\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd704d2a710>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't see a video below, run this cell.\n",
    "# YouTube = True  # Uncomment to get YouTube videos instead of TIB AV.\n",
    "from IPython.display import IFrame\n",
    "IFrame(\"https://www.youtube.com/embed/9WyrWJcvneE\" if \"YouTube\" in globals() else \"//av.tib.eu/player/49032\",\n",
    "       width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2de8ce385e1a4bf4e25e6948ced6e8ee",
     "grade": false,
     "grade_id": "cell-10db7e35b3486095",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### A first question (2 Points).\n",
    "I have to admit there is a bug in the video. If you have a normal distributed random variable $X$, then the probability for $X$ being between $\\mu-\\sigma$ and $\\mu+\\sigma$ is 0.68 (the number stated in the video). However, if $X$ is a multivariate distribution, the probability for being inside the \"one-sigma error ellipsoid\" will be smaller. For example, in our case $X=(x, y, \\theta)\\in\\mathbb{R}^3$, so $X$ is a three-dimensional (multivariate) random variable. For simplicity, assume the covariance matrix to be the identity matrix, so the error ellipsoid would be a ball of radius 1. Can you figure out what the probability is for $X$ being inside this ball? (Hint: *Inside the ball* means $x^2 + y^2 + \\theta^2 \\leq 1$, so you could define $Y=x^2 + y^2 + \\theta^2$ and determine the probability $\\mbox{Pr}(Y\\leq 1)$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "902a3580808e080fef2cf5a7dc5b20a2",
     "grade": false,
     "grade_id": "cell-2aa80984f08cf37f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# What is the probability for X being inside a ball of radius 1, when X is a\n",
    "# 3D standard normal distributed variable? Give it as an integer percentage (rounded to the nearest integer).\n",
    "percentage_inside_ball = 20  # That would be true for 1D, replace by your guess for the 3D case.\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ca7c0b415640f7cc3f5f2f8e87e3dd2",
     "grade": true,
     "grade_id": "cell-3bf7cebfd0764c46",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from hashlib import shake_128\n",
    "def public_string_test(the_answer_string, reference):\n",
    "    m = shake_128()\n",
    "    m.update(the_answer_string.encode())\n",
    "    return m.hexdigest(4) == reference\n",
    "assert public_string_test((\"percentage=%d\" % percentage_inside_ball), '437038cf'), \"Oh no, your answer is wrong!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5fe9fdc862896a2ec8b8d8d349edbd1",
     "grade": false,
     "grade_id": "cell-57c7769bca52898a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### A second question (2 Points).\n",
    "What is the *space* complexity of the EKF SLAM and the presented Particle Filter (PF) SLAM with respect to the number of landmarks $N$?\n",
    "- A: Both are linear in $N$\n",
    "- B: Both are quadratic in $N$\n",
    "- C: EKF is quadratic, PF is linear in $N$\n",
    "- D: EKF is linear, PF is quadratic in $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa9ea6ab736e6b1d7041c2071df2790d",
     "grade": false,
     "grade_id": "cell-f58b9401f3879883",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "the_space_complexity_is = \"C\"  # Replace by your answer.\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d253bf9013dd5e98a821e24883473775",
     "grade": true,
     "grade_id": "cell-79a0dc30c18141c0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from hashlib import shake_128\n",
    "def public_string_test(the_answer_string, reference):\n",
    "    m = shake_128()\n",
    "    m.update(the_answer_string.encode())\n",
    "    return m.hexdigest(4) == reference\n",
    "assert public_string_test(\"cplx=\"+the_space_complexity_is.strip().lower(), '7debab5a'), \"Oh no, your answer is wrong!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1c3737f1c8840a8b84c99a14e742116",
     "grade": false,
     "grade_id": "cell-f72b1c7fffd5d1ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Computing the correspondence likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f68fda076deea7496b871f3c1e7c8ce",
     "grade": false,
     "grade_id": "cell-f13e5c6d553ef6ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"//av.tib.eu/player/49033\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd704ddd750>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't see a video below, run this cell.\n",
    "IFrame(\"https://www.youtube.com/embed/HfBqPi-ydrc\" if \"YouTube\" in globals() else \"//av.tib.eu/player/49033\",\n",
    "       width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74c7fc812c4e90eef7a3dcb0abd15113",
     "grade": false,
     "grade_id": "cell-df1a5e3932fbcbda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The particle class.\n",
    "First, we define a class that represents a single particle. Our filter (defined below) will have many particles to represent the belief. Using a class for the particle makes sense since, as we will learn, there is quite some data and functionality associated with a single particle.\n",
    "\n",
    "In the beginning, however, our particle has only $(x,y,\\theta)$ as member variables, and the only functionality (member functions) are the state transition `g()` (which we know from earlier units) and a `move()` function which simply applies `g()` to itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastSLAM, first version: prediciton only.\n",
    "#\n",
    "# slam_10_a_prediction\n",
    "from lego_robot import *\n",
    "from slam_g_library import get_mean, get_error_ellipse_and_heading_variance,\\\n",
    "     print_particles\n",
    "from math import sin, cos, atan2, exp, pi, sqrt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Particle_1:\n",
    "    def __init__(self, pose):\n",
    "        self.pose = pose\n",
    "\n",
    "    @staticmethod\n",
    "    def g(state, control, w):\n",
    "        \"\"\"State transition. This is exactly the same method as in the Kalman\n",
    "           filter.\"\"\"\n",
    "        x, y, theta = state\n",
    "        l, r = control\n",
    "        if r != l:\n",
    "            alpha = (r - l) / w\n",
    "            rad = l/alpha\n",
    "            g1 = x + (rad + w/2.)*(sin(theta+alpha) - sin(theta))\n",
    "            g2 = y + (rad + w/2.)*(-cos(theta+alpha) + cos(theta))\n",
    "            g3 = (theta + alpha + pi) % (2*pi) - pi\n",
    "        else:\n",
    "            g1 = x + l * cos(theta)\n",
    "            g2 = y + l * sin(theta)\n",
    "            g3 = theta\n",
    "        return np.array([g1, g2, g3])\n",
    "\n",
    "    def move(self, left, right, w):\n",
    "        \"\"\"Given left, right control and robot width, move the robot.\"\"\"\n",
    "        self.pose = self.g(self.pose, (left, right), w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8ea877cc9711d0eccebd7c67c6cd5a4",
     "grade": false,
     "grade_id": "cell-a7cc5a4b200f254f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The particle filter class.\n",
    "We call this class `FastSLAM`. So far, we only implement the prediction step. This is identical to the prediction step we had earlier in Unit E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastSLAM_1:\n",
    "    def __init__(self, initial_particles,\n",
    "                 robot_width, scanner_displacement,\n",
    "                 control_motion_factor, control_turn_factor):\n",
    "        # The particles.\n",
    "        self.particles = initial_particles\n",
    "\n",
    "        # Some constants.\n",
    "        self.robot_width = robot_width\n",
    "        self.scanner_displacement = scanner_displacement\n",
    "        self.control_motion_factor = control_motion_factor\n",
    "        self.control_turn_factor = control_turn_factor\n",
    "\n",
    "    def predict(self, control):\n",
    "        \"\"\"The prediction step of the particle filter.\"\"\"\n",
    "        left, right = control\n",
    "        left_std  = sqrt((self.control_motion_factor * left)**2 +\\\n",
    "                        (self.control_turn_factor * (left-right))**2)\n",
    "        right_std = sqrt((self.control_motion_factor * right)**2 +\\\n",
    "                         (self.control_turn_factor * (left-right))**2)\n",
    "        # Modify list of particles: for each particle, predict its new position.\n",
    "        for p in self.particles:\n",
    "            l = random.gauss(left, left_std)\n",
    "            r = random.gauss(right, right_std)\n",
    "            p.move(l, r, self.robot_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c91b658ceb61c6209fd289b84b64fbfd",
     "grade": false,
     "grade_id": "cell-fa9c7b3334fa7e06",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The main function.\n",
    "Here is the remaining main function. Run it to produce the results from the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Robot constants.\n",
    "    scanner_displacement = 30.0\n",
    "    ticks_to_mm = 0.349\n",
    "    robot_width = 155.0\n",
    "\n",
    "    # Filter constants.\n",
    "    control_motion_factor = 0.35  # Error in motor control.\n",
    "    control_turn_factor = 0.6  # Additional error due to slip when turning.\n",
    "\n",
    "    # Generate initial particles. Each particle is (x, y, theta).\n",
    "    number_of_particles = 25\n",
    "    start_state = np.array([500.0, 0.0, 45.0 / 180.0 * pi])\n",
    "    initial_particles = [Particle_1(start_state.copy())\n",
    "                         for _ in range(number_of_particles)]\n",
    "\n",
    "    # Setup filter.\n",
    "    fs = FastSLAM_1(initial_particles,\n",
    "                    robot_width, scanner_displacement,\n",
    "                    control_motion_factor, control_turn_factor)\n",
    "\n",
    "    # Read data.\n",
    "    logfile = LegoLogfile()\n",
    "    logfile.read(\"robot4_motors.txt\")\n",
    "\n",
    "    # Loop over all motor tick records.\n",
    "    # This is the FastSLAM filter loop, prediction only.\n",
    "    f = open(\"fast_slam_prediction.txt\", \"w\")\n",
    "    for i in range(len(logfile.motor_ticks)):\n",
    "        # Prediction.\n",
    "        control = [x * ticks_to_mm for x in logfile.motor_ticks[i]]\n",
    "        fs.predict(control)\n",
    "\n",
    "        # Output particles.\n",
    "        print_particles(fs.particles, f)\n",
    "\n",
    "        # Output state estimated from all particles.\n",
    "        mean = get_mean(fs.particles)\n",
    "        print(\"F %.0f %.0f %.3f\" %\\\n",
    "              (mean[0] + scanner_displacement * cos(mean[2]),\n",
    "               mean[1] + scanner_displacement * sin(mean[2]),\n",
    "               mean[2]), file=f)\n",
    "\n",
    "        # Output error ellipse and standard deviation of heading.\n",
    "        errors = get_error_ellipse_and_heading_variance(fs.particles, mean)\n",
    "        print(\"E %.3f %.0f %.0f %.3f\" % errors, file=f)\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "285828f17a796fe2978df109f8b584e8",
     "grade": false,
     "grade_id": "cell-e132646d3fc97de4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's have a look at the prediction.\n",
    "As there is no landmark handling so far, there is no correction, and the result is the same as the prediction-only solution of Unit E.\n",
    "\n",
    "You may run the following cell to compare with the results in the video. Note that each time you run the cell above (main), you will get a different result, because of the random sampling going on in `FastSLAM_1.predict()`. So it may fit more or less to the results shown in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69776614dcf4469d8015ccfed9c4181c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Output(layout=Layout(width='600px')), Output(layout=Layout(width='600px')))), Ou…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute this to run the interactive viewer.\n",
    "import ipy_logfile_viewer as lfv\n",
    "v = lfv.IPYLogfileViewer(files=[\"fast_slam_prediction.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6f6a0377115f424366753d8e0962560",
     "grade": false,
     "grade_id": "cell-3446817631cabe40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Programming assignment: compute the correspondence likelihood (10 Points).\n",
    "Now we move on to the parts you'll have to program.\n",
    "\n",
    "In the following cell, you will have to implement the functions `h_expected_measurement_for_landmark`, `H_Ql_jacobian_and_measurement_covariance_for_landmark`, and `wl_likelihood_of_correspondence`, as indicated. Note each of those is only very few lines of code, as explained in the above video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1474f26c4dd36dc8fa88e6fd509110d",
     "grade": false,
     "grade_id": "cell-4c25e6213b2a09bf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# slam_10_b_correspondence_likelihood\n",
    "class Particle_2(Particle_1):\n",
    "    def __init__(self, pose):\n",
    "        super().__init__(pose)\n",
    "        self.landmark_positions = []\n",
    "        self.landmark_covariances = []\n",
    "\n",
    "    def number_of_landmarks(self):\n",
    "        \"\"\"Utility: return current number of landmarks in this particle.\"\"\"\n",
    "        return len(self.landmark_positions)\n",
    "\n",
    "    @staticmethod\n",
    "    def h(state, landmark, scanner_displacement):\n",
    "        \"\"\"Measurement function. Takes a (x, y, theta) state and a (x, y)\n",
    "           landmark, and returns the corresponding (range, bearing).\"\"\"\n",
    "        dx = landmark[0] - (state[0] + scanner_displacement * cos(state[2]))\n",
    "        dy = landmark[1] - (state[1] + scanner_displacement * sin(state[2]))\n",
    "        r = sqrt(dx * dx + dy * dy)\n",
    "        alpha = (atan2(dy, dx) - state[2] + pi) % (2*pi) - pi\n",
    "        return np.array([r, alpha])\n",
    "\n",
    "    @staticmethod\n",
    "    def dh_dlandmark(state, landmark, scanner_displacement):\n",
    "        \"\"\"Derivative with respect to the landmark coordinates. This is related\n",
    "           to the dh_dstate function we used earlier (it is:\n",
    "           -dh_dstate[0:2,0:2]).\"\"\"\n",
    "        theta = state[2]\n",
    "        cost, sint = cos(theta), sin(theta)\n",
    "        dx = landmark[0] - (state[0] + scanner_displacement * cost)\n",
    "        dy = landmark[1] - (state[1] + scanner_displacement * sint)\n",
    "        q = dx * dx + dy * dy\n",
    "        sqrtq = sqrt(q)\n",
    "        dr_dmx = dx / sqrtq\n",
    "        dr_dmy = dy / sqrtq\n",
    "        dalpha_dmx = -dy / q\n",
    "        dalpha_dmy =  dx / q\n",
    "\n",
    "        return np.array([[dr_dmx, dr_dmy],\n",
    "                         [dalpha_dmx, dalpha_dmy]])\n",
    "\n",
    "    def h_expected_measurement_for_landmark(self, landmark_number,\n",
    "                                            scanner_displacement):\n",
    "        \"\"\"Returns the expected distance and bearing measurement for a given\n",
    "           landmark number and the pose of this particle.\"\"\"\n",
    "        # Note: This is just one line of code!\n",
    "        # Hints:\n",
    "        # - the static function h() computes the desired value\n",
    "        # - the state is the robot's pose\n",
    "        # - the landmark is taken from self.landmark_positions.\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        meas = self.h(self.pose, self.landmark_positions[landmark_number], scanner_displacement)\n",
    "        \n",
    "        return meas\n",
    "  \n",
    "        #raise NotImplementedError()\n",
    "    \n",
    "    def H_Ql_jacobian_and_measurement_covariance_for_landmark(\n",
    "        self, landmark_number, Qt_measurement_covariance, scanner_displacement):\n",
    "        \"\"\"Computes Jacobian H of measurement function at the particle's\n",
    "           position and the landmark given by landmark_number. Also computes the\n",
    "           measurement covariance matrix.\"\"\"\n",
    "        # Hints:\n",
    "        # - H is computed using dh_dlandmark.\n",
    "        # - To compute Ql, you will need the product of two matrices,\n",
    "        #   which is A @ B (or alternatively, np.dot(A, B)).\n",
    "        #H = np.eye(2)  # Replace this.\n",
    "        #Ql = np.eye(2)  # Replace this.\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        H = self.dh_dlandmark(self.pose, self.landmark_positions[landmark_number], scanner_displacement)\n",
    "        Ql = np.dot(H, np.dot(self.landmark_covariances[landmark_number], H.T)) + Qt_measurement_covariance\n",
    "        \n",
    " \n",
    "        #raise NotImplementedError()\n",
    "        return (H, Ql)\n",
    "\n",
    "    def wl_likelihood_of_correspondence(self, measurement,\n",
    "                                        landmark_number,\n",
    "                                        Qt_measurement_covariance,\n",
    "                                        scanner_displacement):\n",
    "        \"\"\"For a given measurement and landmark_number, returns the likelihood\n",
    "           that the measurement corresponds to the landmark.\"\"\"\n",
    "        # Hints:\n",
    "        # - You will need delta_z, which is the measurement minus the\n",
    "        #   expected_measurement_for_landmark()\n",
    "        # - IMPORTANT: Correct the angle component of delta_z to prevent\n",
    "        #   multiples of 2 pi! Use something like:\n",
    "        #   delta_z[1] = (delta_z[1]+np.pi) % (2*np.pi) - np.pi\n",
    "        # - Ql is obtained using a call to\n",
    "        #   H_Ql_jacobian_and_measurement_covariance_for_landmark(). You\n",
    "        #   will only need Ql, not H\n",
    "        # - np.linalg.det(A) computes the determinant of A\n",
    "        # - np.linalg.inv(A) computes the inverse of A.\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "        z = measurement\n",
    "        z_expected = self.h_expected_measurement_for_landmark(landmark_number, scanner_displacement)\n",
    "        delta_z = z - z_expected\n",
    "        delta_z[1] = (delta_z[1]+np.pi) % (2*np.pi) - np.pi\n",
    "        H, Ql = self.H_Ql_jacobian_and_measurement_covariance_for_landmark(landmark_number, Qt_measurement_covariance, scanner_displacement)\n",
    "        \n",
    "        a = np.dot(delta_z.T, np.dot(np.linalg.inv(Ql), delta_z))\n",
    "        \n",
    "        l = 1.0/(2.0*pi*sqrt(np.linalg.det(Ql)))*exp(-0.5*a)\n",
    "        return l\n",
    "\n",
    "    def compute_correspondence_likelihoods(self, measurement,\n",
    "                                           number_of_landmarks,\n",
    "                                           Qt_measurement_covariance,\n",
    "                                           scanner_displacement):\n",
    "        \"\"\"For a given measurement, returns a list of all correspondence\n",
    "           likelihoods (from index 0 to number_of_landmarks-1).\"\"\"\n",
    "        likelihoods = []\n",
    "        for i in range(number_of_landmarks):\n",
    "            likelihoods.append(\n",
    "                self.wl_likelihood_of_correspondence(\n",
    "                    measurement, i, Qt_measurement_covariance,\n",
    "                    scanner_displacement))\n",
    "        return likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "abee75304ed201b33440b94fcce3e28d",
     "grade": false,
     "grade_id": "cell-5de1b9cddc8b4a49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the following cell and compare the output to the one shown in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landmark 0 ----------\n",
      " Expected range: 707.1067811865476 bearing [deg] -45.0\n",
      " Covariance of measurement:\n",
      " [[5.00000000e+04 0.00000000e+00]\n",
      " [3.07551183e-16 8.85389195e-02]]\n",
      "Landmark 1 ----------\n",
      " Expected range: 1000.0 bearing [deg] 0.0\n",
      " Covariance of measurement:\n",
      " [[5.00000000e+04 0.00000000e+00]\n",
      " [0.00000000e+00 7.85389195e-02]]\n",
      "Landmark 2 ----------\n",
      " Expected range: 2000.0 bearing [deg] 0.0\n",
      " Covariance of measurement:\n",
      " [[ 8.00000000e+04 -5.00000000e+00]\n",
      " [-5.00000000e+00  7.85389195e-02]]\n",
      "Measurement likelihoods ----------\n",
      "Likelihoods for measurement close to landmark 0\n",
      "[0.0023920377034520276, 2.1219589862494533e-05, 4.82626177148495e-10]\n",
      "Likelihoods for measurement exactly between landmark 1 and 2\n",
      "[1.3664163824358227e-07, 0.00020847619108492658, 0.00041908533374704956]\n"
     ]
    }
   ],
   "source": [
    "def add_landmarks(particle):\n",
    "    \"\"\"Helper function to add some landmarks and their covariances.\"\"\"\n",
    "    # Add a landmark, at (500,-500), with standard deviation 100.\n",
    "    particle.landmark_positions.append(np.array([500.0, -500.0]))\n",
    "    particle.landmark_covariances.append(np.array(\n",
    "        [[ 100.0**2,   0.0 ],\n",
    "         [   0.0, 100.0**2 ]]))\n",
    "    # Add two landmarks along the x axis, at 1000 and 2000, with\n",
    "    # different covariances.\n",
    "    particle.landmark_positions.append(np.array([1000.0, 0.0]))\n",
    "    particle.landmark_covariances.append(np.array(\n",
    "        [[ 100.0**2,   0.0 ],\n",
    "         [   0.0, 100.0**2 ]]))\n",
    "    particle.landmark_positions.append(np.array([2000.0, 0.0]))\n",
    "    particle.landmark_covariances.append(np.array(\n",
    "        [[ 200.0**2, -100.0**2 ],\n",
    "         [-100.0**2,  200.0**2 ]]))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Robot constants.\n",
    "    scanner_displacement = 30.0\n",
    "\n",
    "    # Filter constants.\n",
    "    measurement_distance_stddev = 200.0  # Distance measurement error of cylinders.\n",
    "    measurement_angle_stddev = 15.0 / 180.0 * pi  # Angle measurement error.\n",
    "    Qt_measurement_covariance = \\\n",
    "        np.diag([measurement_distance_stddev**2,\n",
    "                 measurement_angle_stddev**2])\n",
    "\n",
    "    # Define a particle: position (x, y) and orientation.\n",
    "    p = Particle_2(np.array([-scanner_displacement, 0.0, 0.0]))\n",
    "    add_landmarks(p)\n",
    "    N = p.number_of_landmarks()\n",
    "\n",
    "    # Compute expected measurements.\n",
    "    for i in range(N):\n",
    "        print(\"Landmark\", i, \"----------\")\n",
    "        em = p.h_expected_measurement_for_landmark(i, scanner_displacement)\n",
    "        print(\" Expected range:\", em[0], \"bearing [deg]\", em[1]/pi*180)           \n",
    "        H, Ql = p.H_Ql_jacobian_and_measurement_covariance_for_landmark(\n",
    "            i, Qt_measurement_covariance, scanner_displacement)\n",
    "        print(\" Covariance of measurement:\\n\", Ql)\n",
    "\n",
    "    # Compute correspondence likelihoods.\n",
    "    # Define a set of measurements: (range, bearing).\n",
    "    print(\"Measurement likelihoods ----------\")\n",
    "    measurements = [\n",
    "        (\"close to landmark 0\", np.array([500*sqrt(2), -pi/4])),\n",
    "        (\"exactly between landmark 1 and 2\", np.array([1500.0, 0.0]))\n",
    "        ]\n",
    "    for (text, m) in measurements:\n",
    "        print(\"Likelihoods for measurement\", text)\n",
    "        likelihoods = p.compute_correspondence_likelihoods(\n",
    "            m, N, Qt_measurement_covariance, scanner_displacement)\n",
    "        print(likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58d114e1a7ff8b67a7e42c5f540c340b",
     "grade": false,
     "grade_id": "cell-276b1b871b384197",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, the test. If the above values are like in the video, this should pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67284784e938933457b4b4cfb2a57d97",
     "grade": true,
     "grade_id": "cell-437ea563ad8e5915",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from math import pi, sqrt\n",
    "import numpy as np\n",
    "from hashlib import shake_128\n",
    "\n",
    "def test(the_particle_class):\n",
    "    # Robot constants.\n",
    "    scanner_displacement = 30.0\n",
    "\n",
    "    # Filter constants.\n",
    "    measurement_distance_stddev = 200.0  # Distance measurement error of cylinders.\n",
    "    measurement_angle_stddev = 15.0 / 180.0 * pi  # Angle measurement error.\n",
    "    Qt_measurement_covariance = \\\n",
    "        np.diag([measurement_distance_stddev**2,\n",
    "                 measurement_angle_stddev**2])\n",
    "\n",
    "    # Define a particle: position (x, y) and orientation.\n",
    "    p = the_particle_class(np.array([-scanner_displacement, 0.0, 0.0]))\n",
    "    p.landmark_positions.append(np.array([501.0, -502.0]))\n",
    "    p.landmark_covariances.append(np.array(\n",
    "        [[ 102.0**2,   0.0 ],\n",
    "         [   0.0, 104.0**2 ]]))\n",
    "    p.landmark_positions.append(np.array([1010.0, 0.0]))\n",
    "    p.landmark_covariances.append(np.array(\n",
    "        [[ 106.0**2,   0.0 ],\n",
    "         [   0.0, 108.0**2 ]]))\n",
    "    p.landmark_positions.append(np.array([2011.0, 0.0]))\n",
    "    p.landmark_covariances.append(np.array(\n",
    "        [[ 210.0**2, -105.0**2 ],\n",
    "         [-105.0**2,  220.0**2 ]]))\n",
    "    N = p.number_of_landmarks()\n",
    "\n",
    "    # Define a set of measurements: (range, bearing).\n",
    "    measurements = [\n",
    "        np.array([495*sqrt(2), -pi/4]),\n",
    "        np.array([1502.0, 0.123])]\n",
    "    # Compute correspondence likelihoods.\n",
    "    hsh = shake_128()\n",
    "    for m in measurements:\n",
    "        likelihoods = p.compute_correspondence_likelihoods(\n",
    "            m, N, Qt_measurement_covariance, scanner_displacement)\n",
    "        hsh.update((\"%.3e,%.3e,%.3e\" % tuple(likelihoods)).encode())\n",
    "    return hsh.hexdigest(8) == \"b5c9b9793d358adb\"\n",
    "assert test(Particle_2), \"Oh no, it's wrong!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06e7c708fe2c269d471f02c7081b7666",
     "grade": false,
     "grade_id": "cell-1eec8fb3876c78d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Insertion of a new landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4d608ba26a3cec0d591aa8171a993b6",
     "grade": false,
     "grade_id": "cell-1bfa13686e7808af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"//av.tib.eu/player/49034\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd738040250>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't see a video below, run this cell.\n",
    "IFrame(\"https://www.youtube.com/embed/Hk5s-8-0RlM\" if \"YouTube\" in globals() else \"//av.tib.eu/player/49034\",\n",
    "       width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ff0a106ed7b33f5ce2d97013d5839a0",
     "grade": false,
     "grade_id": "cell-30aebbf14cec4d20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Programming assignment: insertion of a new landmark (10 Points).\n",
    "Note that you don't have to copy any previous code from above into your solution, neither here nor in any other cells of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4f3c031495cc7ebb644d15c7375985e",
     "grade": false,
     "grade_id": "cell-6991ddcd98a61979",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landmark 0 ----------\n",
      "Position: [1000.    0.]\n",
      "Landmark covariance:\n",
      " [[40000.             0.        ]\n",
      " [    0.         68538.91945201]]\n",
      "This corresponds to the error ellipse:\n",
      "Angle [deg]: 0.0\n",
      "Axis 1: 200.0\n",
      "Axis 2: 261.79938779914943\n",
      "Landmark 1 ----------\n",
      "Position: [2000.    0.]\n",
      "Landmark covariance:\n",
      " [[ 40000.              0.        ]\n",
      " [     0.         274155.67780804]]\n",
      "This corresponds to the error ellipse:\n",
      "Angle [deg]: 0.0\n",
      "Axis 1: 200.0\n",
      "Axis 2: 523.5987755982989\n",
      "Landmark 2 ----------\n",
      "Position: [707.10678119 707.10678119]\n",
      "Landmark covariance:\n",
      " [[ 54269.459726 -14269.459726]\n",
      " [-14269.459726  54269.459726]]\n",
      "This corresponds to the error ellipse:\n",
      "Angle [deg]: -45.0\n",
      "Axis 1: 261.7993877991494\n",
      "Axis 2: 199.99999999999997\n"
     ]
    }
   ],
   "source": [
    "# slam_10_c_new_landmark\n",
    "from lego_robot import *\n",
    "from math import sin, cos, pi, atan2, sqrt\n",
    "import numpy as np\n",
    "\n",
    "class Particle_3(Particle_2):\n",
    "    def initialize_new_landmark(self, measurement_in_scanner_system,\n",
    "                                Qt_measurement_covariance,\n",
    "                                scanner_displacement):\n",
    "        \"\"\"Given a (x, y) measurement in the scanner's system, initializes a\n",
    "           new landmark and its covariance.\"\"\"\n",
    "        scanner_pose = (self.pose[0] + cos(self.pose[2]) * scanner_displacement,\n",
    "                        self.pose[1] + sin(self.pose[2]) * scanner_displacement,\n",
    "                        self.pose[2])\n",
    "        # Hints:\n",
    "        # - LegoLogfile.scanner_to_world() (from lego_robot.py) will return\n",
    "        #   the world coordinate, given the scanner pose and the coordinate in\n",
    "        #   the scanner's system.\n",
    "        # - H is obtained from dh_dlandmark()\n",
    "        # - Use np.linalg.inv(A) to invert a matrix A\n",
    "        # - Remember the matrix product is A @ B, or np.dot(A,B).\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        landmark_pos = LegoLogfile.scanner_to_world(scanner_pose, measurement_in_scanner_system)\n",
    "        H = self.dh_dlandmark(self.pose, landmark_pos, scanner_displacement)\n",
    "        landmark_cov = np.dot(np.linalg.inv(H), np.dot(Qt_measurement_covariance, np.linalg.inv(H).T))\n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "        self.landmark_positions.append(np.array(landmark_pos))\n",
    "        self.landmark_covariances.append(landmark_cov)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    # Robot constants.\n",
    "    scanner_displacement = 30.0\n",
    "\n",
    "    # Filter constants.\n",
    "    measurement_distance_stddev = 200.0  # Distance measurement error of cylinders.\n",
    "    measurement_angle_stddev = 15.0 / 180.0 * pi  # Angle measurement error.\n",
    "    Qt_measurement_covariance = \\\n",
    "        np.diag([measurement_distance_stddev**2,\n",
    "                 measurement_angle_stddev**2])\n",
    "\n",
    "    # Define a particle: position (x, y) and orientation.\n",
    "    p = Particle_3(np.array([-scanner_displacement, 0.0, 0.0]))\n",
    "\n",
    "    # Add a landmark along the x axis.\n",
    "    measurement_in_scanner_system = (1000.0, 0.0)\n",
    "    p.initialize_new_landmark(measurement_in_scanner_system,\n",
    "                              Qt_measurement_covariance,\n",
    "                              scanner_displacement)\n",
    "    # Add another landmark at twice the distance.\n",
    "    measurement_in_scanner_system = (2000.0, 0.0)\n",
    "    p.initialize_new_landmark(measurement_in_scanner_system,\n",
    "                              Qt_measurement_covariance,\n",
    "                              scanner_displacement)\n",
    "    # Add another landmark at the distance of the first landmark, but at\n",
    "    # a bearing angle of 45 degrees.\n",
    "    measurement_in_scanner_system = np.array([1000.0, 1000.0]) / sqrt(2)\n",
    "    p.initialize_new_landmark(measurement_in_scanner_system,\n",
    "                              Qt_measurement_covariance,\n",
    "                              scanner_displacement)\n",
    "\n",
    "    # Print all landmarks.\n",
    "    for i in range(p.number_of_landmarks()):\n",
    "        print(\"Landmark\", i, \"----------\")\n",
    "        print(\"Position:\", p.landmark_positions[i])\n",
    "        print(\"Landmark covariance:\\n\", p.landmark_covariances[i])\n",
    "        print(\"This corresponds to the error ellipse:\")\n",
    "        eigenvals, eigenvects = np.linalg.eig(p.landmark_covariances[i])\n",
    "        angle = atan2(eigenvects[1,0], eigenvects[0,0])\n",
    "        print(\"Angle [deg]:\", angle / pi * 180.0)\n",
    "        print(\"Axis 1:\", sqrt(eigenvals[0]))\n",
    "        print(\"Axis 2:\", sqrt(eigenvals[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fea61c41f4cfef156f6027537e7b4875",
     "grade": false,
     "grade_id": "cell-70a964f882b1763d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now the test. If the values above are all good, this should also pass easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38721b1639aa0fd18fe8222f8de623ae",
     "grade": true,
     "grade_id": "cell-bb53a6d2d8103317",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from math import pi, sqrt\n",
    "import numpy as np\n",
    "from hashlib import shake_128\n",
    "\n",
    "def test(the_particle_class):\n",
    "    # Robot constants.\n",
    "    scanner_displacement = 30.0\n",
    "    # Filter constants.\n",
    "    measurement_distance_stddev = 200.0  # Distance measurement error of cylinders.\n",
    "    measurement_angle_stddev = 15.0 / 180.0 * pi  # Angle measurement error.\n",
    "    Qt_measurement_covariance = \\\n",
    "        np.diag([measurement_distance_stddev**2,\n",
    "                 measurement_angle_stddev**2])\n",
    "    # Define a particle: position (x, y) and orientation.\n",
    "    p = the_particle_class(np.array([-25.0, 12.0, 0.123]))\n",
    "    # Add landmarks.\n",
    "    measurements_in_scanner_system = [(1005.0, 6.7), (2012.0, -8.9),\n",
    "        np.array([1006.0, 1006.0]) / sqrt(2)]\n",
    "    for m in measurements_in_scanner_system:\n",
    "        p.initialize_new_landmark(m, Qt_measurement_covariance,\n",
    "                                  scanner_displacement)\n",
    "    # Check.\n",
    "    hsh = shake_128()\n",
    "    for pos in p.landmark_positions:\n",
    "        hsh.update((\"%.3e,%.3e\" % tuple(pos)).encode())\n",
    "    for cov in p.landmark_covariances:\n",
    "        hsh.update((\"%.3e,%.3e,%.3e,%.3e\" % tuple(cov.flatten())).encode())\n",
    "    return hsh.hexdigest(8) == \"3b2cd61d10476d29\"\n",
    "assert test(Particle_3), \"Oh no, it's wrong!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3860a5da44dfcdf50931563ceb43fba8",
     "grade": false,
     "grade_id": "cell-9d39b413c2a39767",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Update of an existing landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1687d1c6a10a4b6053c1ef3186d88e0f",
     "grade": false,
     "grade_id": "cell-6c52970890f16c1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"//av.tib.eu/player/49035\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd704d4cb10>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't see a video below, run this cell.\n",
    "IFrame(\"https://www.youtube.com/embed/PZe_aAgIxHA\" if \"YouTube\" in globals() else \"//av.tib.eu/player/49035\",\n",
    "       width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b45f8405b701dce17790259fe345653",
     "grade": false,
     "grade_id": "cell-1b620dfe893c8e05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Program assignment: update of an existing landmark (10 Points).\n",
    "Note again that while the video explains that you'll have to copy old code from above, this is not necessary in this notebook version of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c40ec63f9c41c7583b7d6d2c51c18a7",
     "grade": false,
     "grade_id": "cell-ca1a2f890430c9b1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landmarks - before update:\n",
      "Landmark 0 ----------\n",
      " Position: [1000.    0.]\n",
      " Error ellipse:\n",
      "  Angle [deg]: 0.0\n",
      "  Axis 1: 200.0\n",
      "  Axis 2: 261.7993877991505\n",
      "Landmark 1 ----------\n",
      " Position: [2000.    0.]\n",
      " Error ellipse:\n",
      "  Angle [deg]: 0.0\n",
      "  Axis 1: 200.0\n",
      "  Axis 2: 523.598775598301\n",
      "Landmark 2 ----------\n",
      " Position: [707.10678119 707.10678119]\n",
      " Error ellipse:\n",
      "  Angle [deg]: -45.0\n",
      "  Axis 1: 261.7993877991314\n",
      "  Axis 2: 200.0\n",
      "\n",
      "Landmarks - after update:\n",
      "Landmark 0 ----------\n",
      " Position: [1000.    0.]\n",
      " Error ellipse:\n",
      "  Angle [deg]: 0.0\n",
      "  Axis 1: 141.4213562373095\n",
      "  Axis 2: 185.1201224232656\n",
      "Landmark 1 ----------\n",
      " Position: [2050.    0.]\n",
      " Error ellipse:\n",
      "  Angle [deg]: 0.0\n",
      "  Axis 1: 141.4213562373095\n",
      "  Axis 2: 370.2402448465312\n",
      "Landmark 2 ----------\n",
      " Position: [707.10678119 707.10678119]\n",
      " Error ellipse:\n",
      "  Angle [deg]: -45.0\n",
      "  Axis 1: 261.7993877991314\n",
      "  Axis 2: 200.0\n"
     ]
    }
   ],
   "source": [
    "# slam_10_d_update_landmark\n",
    "from math import pi, atan2, sqrt\n",
    "import numpy as np\n",
    "\n",
    "class Particle_4(Particle_3):\n",
    "    def update_landmark(self, landmark_number, measurement,\n",
    "                        Qt_measurement_covariance, scanner_displacement):\n",
    "        \"\"\"Update a landmark's estimated position and covariance.\"\"\"\n",
    "        # Hints:\n",
    "        # - H and Ql can be computed using\n",
    "        #   H_Ql_jacobian_and_measurement_covariance_for_landmark()\n",
    "        # - Use np.linalg.inv(A) to compute the inverse of A\n",
    "        # - delta_z is measurement minus expected measurement\n",
    "        # - IMPORTANT: Correct the angle component of delta_z to prevent\n",
    "        #   multiples of 2 pi! Use something like:\n",
    "        #   delta_z[1] = (delta_z[1]+np.pi) % (2*np.pi) - np.pi\n",
    "        # - Expected measurement can be computed using\n",
    "        #   h_expected_measurement_for_landmark()\n",
    "        # - Remember to update landmark_positions[landmark_number] as well\n",
    "        #   as landmark_covariances[landmark_number].\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        H, Ql = self.H_Ql_jacobian_and_measurement_covariance_for_landmark( landmark_number,Qt_measurement_covariance,scanner_displacement )\n",
    "        \n",
    "        # - Use np.linalg.inv(A) to compute the inverse of A\n",
    "        Ql_inv = np.linalg.inv(Ql)\n",
    "        \n",
    "        # - Delta z is measurement minus expected measurement\n",
    "        # - Expected measurement can be computed using\n",
    "        #   h_expected_measurement_for_landmark()\n",
    "        h = self.h_expected_measurement_for_landmark( landmark_number, scanner_displacement )\n",
    "        delta_z = measurement - h\n",
    "        delta_z[1] = (delta_z[1]+np.pi) % (2*np.pi) - np.pi\n",
    "\n",
    "        # - Remember to update landmark_positions[landmark_number] as well\n",
    "        #   as landmark_covariances[landmark_number].\n",
    "        Sigma_old = self.landmark_covariances[landmark_number]\n",
    "        K = np.dot(np.dot(Sigma_old,H.T),Ql_inv)\n",
    "        KH = np.dot(K,H)\n",
    "        \n",
    "        self.landmark_positions[landmark_number] += np.dot(K, delta_z)\n",
    "        self.landmark_covariances[landmark_number] = np.dot((np.eye(KH.shape[0]) - KH), Sigma_old)\n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "\n",
    "def insert_landmarks(particle):\n",
    "    \"\"\"Insert the landmarks from the slam_10_c_new_landmark exercise.\"\"\"\n",
    "    positions = [\n",
    "        np.array([1000.0, 0.0]),\n",
    "        np.array([2000.0, 0.0]),\n",
    "        np.array([707.10678118654744, 707.10678118654744])\n",
    "        ]\n",
    "    covariances = [\n",
    "        np.array([[ 40000., 0.],\n",
    "                  [ 0., 68538.91945201]]),\n",
    "        np.array([[ 40000., 0.],\n",
    "                  [ 0., 274155.67780804]]),\n",
    "        np.array([[ 54269.459726, -14269.459726],\n",
    "                  [ -14269.459726,54269.459726]])\n",
    "        ]\n",
    "    particle.landmark_positions.extend(positions)\n",
    "    particle.landmark_covariances.extend(covariances)\n",
    "\n",
    "def print_landmarks(particle):\n",
    "    for i in range(particle.number_of_landmarks()):\n",
    "        print(\"Landmark\", i, \"----------\")\n",
    "        print(\" Position:\", particle.landmark_positions[i])\n",
    "        print(\" Error ellipse:\")\n",
    "        eigenvals, eigenvects = np.linalg.eig(particle.landmark_covariances[i])\n",
    "        angle = atan2(eigenvects[1,0], eigenvects[0,0])\n",
    "        print(\"  Angle [deg]:\", angle / pi * 180.0)\n",
    "        print(\"  Axis 1:\", sqrt(eigenvals[0]))\n",
    "        print(\"  Axis 2:\", sqrt(eigenvals[1]))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Robot constants.\n",
    "    scanner_displacement = 30.0\n",
    "\n",
    "    # Filter constants.\n",
    "    measurement_distance_stddev = 200.0  # Distance measurement error of cylinders.\n",
    "    measurement_angle_stddev = 15.0 / 180.0 * pi  # Angle measurement error.\n",
    "    Qt_measurement_covariance = \\\n",
    "        np.diag([measurement_distance_stddev**2,\n",
    "                 measurement_angle_stddev**2])\n",
    "\n",
    "    # Define a particle: position (x, y) and orientation.\n",
    "    p = Particle_4(np.array([-scanner_displacement, 0.0, 0.0]))\n",
    "\n",
    "    # Insert the landmarks from the slam_10_c_new_landmark exercise.\n",
    "    insert_landmarks(p)\n",
    "\n",
    "    # Print all landmarks (before update).\n",
    "    print(\"Landmarks - before update:\")\n",
    "    print_landmarks(p)\n",
    "\n",
    "    # Measure first landmark.\n",
    "    # Assume we would measure the exact distance (1000) and bearing (0.0).\n",
    "    measurement = np.array([1000.0, 0.0])\n",
    "    p.update_landmark(0, measurement, Qt_measurement_covariance,\n",
    "                      scanner_displacement)\n",
    "\n",
    "    # Measure second landmark.\n",
    "    # Assume we would measure a slightly different range.\n",
    "    measurement = np.array([2000.0 + 100.0, 0.0])\n",
    "    p.update_landmark(1, measurement, Qt_measurement_covariance,\n",
    "                      scanner_displacement)\n",
    "\n",
    "    # Print all landmarks (after update).\n",
    "    print(\"\\nLandmarks - after update:\")\n",
    "    print_landmarks(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fc35b9f1caddc3a912851f0ce357752",
     "grade": false,
     "grade_id": "cell-fd1ac0046969e7eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now the test. If the values above are all like in the video, this should also pass easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90ac0c2a97e5dbdce1f163da93fe8db0",
     "grade": true,
     "grade_id": "cell-c328cfcf2b8431b3",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from math import pi, sqrt\n",
    "import numpy as np\n",
    "from hashlib import shake_128\n",
    "\n",
    "def test(the_particle_class):\n",
    "    # Robot constants.\n",
    "    scanner_displacement = 30.0\n",
    "    # Filter constants.\n",
    "    measurement_distance_stddev = 200.0  # Distance measurement error of cylinders.\n",
    "    measurement_angle_stddev = 15.0 / 180.0 * pi  # Angle measurement error.\n",
    "    Qt_measurement_covariance = \\\n",
    "        np.diag([measurement_distance_stddev**2,\n",
    "                 measurement_angle_stddev**2])\n",
    "    # Define a particle: position (x, y) and orientation.\n",
    "    p = the_particle_class(np.array([-27, 1.23, -0.123]))\n",
    "    # Insert the landmarks from the slam_10_c_new_landmark exercise.\n",
    "    p.landmark_positions.extend(np.array(l) for l in [(1000.0, 0.0),\n",
    "        (2000.0, 0.0), (707.10678118654744, 707.10678118654744)])\n",
    "    p.landmark_covariances.extend(np.array(l) for l in [\n",
    "        [[ 40000., 0.],\n",
    "         [ 0., 68538.91945201]],\n",
    "        [[ 40000., 0.],\n",
    "         [ 0., 274155.67780804]],\n",
    "        [[ 54269.459726, -14269.459726],\n",
    "         [ -14269.459726, 54269.459726]] ])\n",
    "    # Measurements.\n",
    "    measurements = [np.array(m) for m in [\n",
    "        (1000.0, 12.3), (2000.0 + 100.0, 0.0),\n",
    "        (1034.0, 45.0 / 180.0 * pi)]]\n",
    "    for i, m in enumerate(measurements):\n",
    "        p.update_landmark(i, m, Qt_measurement_covariance,\n",
    "                          scanner_displacement)\n",
    "    # Check.\n",
    "    hsh = shake_128()\n",
    "    for pos in p.landmark_positions:\n",
    "        hsh.update((\"%.3e,%.3e\" % tuple(pos)).encode())\n",
    "    for cov in p.landmark_covariances:\n",
    "        eigenvals, eigenvects = np.linalg.eig(cov)\n",
    "        angle = np.arctan2(eigenvects[1,0], eigenvects[0,0]) / pi * 180.0\n",
    "        hsh.update((\"%.3e,%.3e,%.3e\" % ((angle,) + tuple(np.sqrt(eigenvals)))).\n",
    "                   encode())\n",
    "    return hsh.hexdigest(8) == \"ca295d0daf3f08cc\"\n",
    "assert test(Particle_4), \"Oh no, it's wrong!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb04cbcfade413ab025864431a6acfc8",
     "grade": false,
     "grade_id": "cell-0425ebcfc0e8c6ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Putting it together: prediction and correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25074e78f0e4b21904e19cb0fed4ccd5",
     "grade": false,
     "grade_id": "cell-4a8b3c59bc28a4b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"//av.tib.eu/player/49036\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd704b22c90>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't see a video below, run this cell.\n",
    "IFrame(\"https://www.youtube.com/embed/50NqWjiMnnE\" if \"YouTube\" in globals() else \"//av.tib.eu/player/49036\",\n",
    "       width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5471b91bf7e63e040381a5afd324dab",
     "grade": false,
     "grade_id": "cell-f192c1fc7af2117b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Programming assignment: the `update_particle` function (10 Points).\n",
    "When given a particle and a measurement, this implements the functionality of either inserting a new landmark in the particle's list of landmarks, or updating the EKF for an existing landmark. Note that, as explained above, there is a list of landmarks *for each particle*, because in SLAM, each particle describes the robot's state *and* the map. Therefore, given a measurement at a certain time step, this may lead to the update of a landmark in one particle, while at the same time forcing the creation of a new landmark in another particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4e82d3041fd5bf052b0e4a4d96096fb",
     "grade": false,
     "grade_id": "cell-b20e5fd8d453d2a2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# slam_10_e_correction\n",
    "class Particle_5(Particle_4):\n",
    "    def update_particle(self, measurement, measurement_in_scanner_system,\n",
    "                        number_of_landmarks,\n",
    "                        minimum_correspondence_likelihood,\n",
    "                        Qt_measurement_covariance, scanner_displacement):\n",
    "        \"\"\"Given a measurement, computes the likelihood that it belongs to any\n",
    "           of the landmarks in the particle. If there are none, or if all\n",
    "           likelihoods are below the minimum_correspondence_likelihood\n",
    "           threshold, add a landmark to the particle. Otherwise, update the\n",
    "           (existing) landmark with the largest likelihood.\"\"\"\n",
    "        # Here is a hint on the overall structure.\n",
    "        \n",
    "        # First, compute likelihood of correspondence of measurement to all\n",
    "        # landmarks (from 0 to number_of_landmarks-1).\n",
    "        # likelihoods = ... --->>> use: compute_correspondence_likelihoods().\n",
    "\n",
    "        # If the likelihood list is empty, or the max correspondence likelihood\n",
    "        # is still smaller than minimum_correspondence_likelihood, setup\n",
    "        # a new landmark. Return minimum_correspondence_likelihood in this case.\n",
    "        #if not likelihoods or\\\n",
    "           #max(likelihoods) < minimum_correspondence_likelihood:\n",
    "                # --->>> Add code to insert a new landmark.\n",
    "            #return minimum_correspondence_likelihood\n",
    "\n",
    "        # Else update the particle's EKF for the corresponding landmark, and\n",
    "        # return the likelihood of this landmark.\n",
    "        #else:\n",
    "            # --->>> Add code to find the landmark with the maximum\n",
    "            #        likelihood w and its index.\n",
    "            #        Then, use update_landmark() to update it.\n",
    "            #        Finally, return the likelihood, w.\n",
    "            #return w\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        likelihoods = self.compute_correspondence_likelihoods(measurement, \n",
    "                                                              number_of_landmarks,\n",
    "                                                              Qt_measurement_covariance,\n",
    "                                                              scanner_displacement)\n",
    "\n",
    " \n",
    "        if not likelihoods or max(likelihoods) < minimum_correspondence_likelihood:\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "            self.initialize_new_landmark(measurement_in_scanner_system,\n",
    "                                         Qt_measurement_covariance,\n",
    "                                         scanner_displacement)\n",
    "            \n",
    "            return minimum_correspondence_likelihood\n",
    "\n",
    "\n",
    "        else:\n",
    " \n",
    "            w = max(likelihoods)\n",
    "    \n",
    "            index = likelihoods.index(w)\n",
    "\n",
    "            self.update_landmark(index, measurement, Qt_measurement_covariance, scanner_displacement)\n",
    "            return w\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6de235b72e462fd4fefc280e63a2d3d5",
     "grade": true,
     "grade_id": "cell-36aac3f0493a4b6e",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "def test(the_particle_class):\n",
    "    pose = (68.32884208008977, 76.96542519718776, -2.0080322820184113)\n",
    "    ranges = [948,951,762,746,1263,1263,948,944,1261,746,1259,952,757,751,\n",
    "              944,757,961,949,751,762,955,1249,747,1264,952,960,1263,1254,\n",
    "              945,1253,]\n",
    "    bearings = [2532,2594,-5763,-5615,-1459,-1579,2611,2608,-1528,-5656,-1616,\n",
    "                2489,-5725,-5582,2659,-5749,2521,2665,-5688,-5697,2603,-1545,\n",
    "                -5596,-1587,2492,2488,-1590,-1548,2538,-1555,]\n",
    "    measurements = np.vstack((np.array(ranges),np.array(bearings)/1000.0)).T\n",
    "    landmarks = [\n",
    "    [860.7850186495331, 551.186586663526] ,\n",
    "    [183.08593109670122, -695.7721891748381] ,\n",
    "    [-1094.8945193216762, 563.6871990612207] ,\n",
    "    ]\n",
    "    covariances = [\n",
    "    [4533.008592218122, -1307.6823229452978,\n",
    "     -1307.6823229452978, 5789.986659543876] ,\n",
    "    [5574.2701540879625, 90.89120931942583,\n",
    "     90.89120931942574, 4960.68159631367] ,\n",
    "    [6485.57330717526, 3729.0301012931295,\n",
    "     3729.030101293129, 14015.330427706336] ,\n",
    "    ]\n",
    "    # Feed data to particle class.\n",
    "    p = the_particle_class(pose)\n",
    "    scanner_displacement = 33.0\n",
    "    measurement_distance_stddev = 211.0\n",
    "    measurement_angle_stddev = 17.0 / 180.0 * np.pi\n",
    "    Qt_measurement_covariance = \\\n",
    "        np.diag([measurement_distance_stddev**2,\n",
    "                 measurement_angle_stddev**2])\n",
    "    minimum_correspondence_likelihood = 0.001\n",
    "    for m in measurements:\n",
    "        m_scanner = m[0]*np.cos(m[1]), m[0]*np.sin(m[1])\n",
    "        p.update_particle(m, m_scanner, p.number_of_landmarks(),\n",
    "            minimum_correspondence_likelihood, Qt_measurement_covariance,\n",
    "            scanner_displacement)\n",
    "\n",
    "    # Compare landmark positions and covariances.\n",
    "    for i, pos in enumerate(landmarks):\n",
    "        if not np.allclose(pos, p.landmark_positions[i]):\n",
    "            return False\n",
    "    for i, cov in enumerate(covariances):\n",
    "        if not np.allclose(cov, p.landmark_covariances[i].flatten()):\n",
    "            return False\n",
    "    return True\n",
    "assert test(Particle_5), \"Oh no, it's wrong!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f7ac8a5cb14af51943e6b5353ca8557",
     "grade": false,
     "grade_id": "cell-ed7f8adb127f97ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the following cell to produce the output, which can then be viewed using the logfile viewer.\n",
    "\n",
    "Note that running this make a short while, depending on the number of particles (the default `number_of_particles` is 25, you may (and should!) play with it).\n",
    "\n",
    "Also note that each time you run it, you will of course get a different result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lego_robot import *\n",
    "from slam_g_library import get_cylinders_from_scan, write_cylinders,\\\n",
    "     write_error_ellipses, get_mean, get_error_ellipse_and_heading_variance,\\\n",
    "     print_particles\n",
    "from math import sin, cos, pi\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class FastSLAM_2(FastSLAM_1):\n",
    "    def __init__(self, initial_particles,\n",
    "                 robot_width, scanner_displacement,\n",
    "                 control_motion_factor, control_turn_factor,\n",
    "                 measurement_distance_stddev, measurement_angle_stddev,\n",
    "                 minimum_correspondence_likelihood):\n",
    "        super().__init__(initial_particles, robot_width,\n",
    "            scanner_displacement, control_motion_factor, control_turn_factor)\n",
    "        # Some constants.\n",
    "        self.measurement_distance_stddev = measurement_distance_stddev\n",
    "        self.measurement_angle_stddev = measurement_angle_stddev\n",
    "        self.minimum_correspondence_likelihood = \\\n",
    "            minimum_correspondence_likelihood\n",
    "\n",
    "    def update_and_compute_weights(self, cylinders):\n",
    "        \"\"\"Updates all particles and returns a list of their weights.\"\"\"\n",
    "        Qt_measurement_covariance = \\\n",
    "            np.diag([self.measurement_distance_stddev**2,\n",
    "                     self.measurement_angle_stddev**2])\n",
    "        weights = []\n",
    "        for p in self.particles:\n",
    "            # Loop over all measurements.\n",
    "            number_of_landmarks = p.number_of_landmarks()\n",
    "            weight = 1.0\n",
    "            for measurement, measurement_in_scanner_system in cylinders:\n",
    "                weight *= p.update_particle(\n",
    "                    measurement, measurement_in_scanner_system,\n",
    "                    number_of_landmarks,\n",
    "                    self.minimum_correspondence_likelihood,\n",
    "                    Qt_measurement_covariance, self.scanner_displacement)\n",
    "\n",
    "            # Append overall weight of this particle to weight list.\n",
    "            weights.append(weight)\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def resample(self, weights):\n",
    "        \"\"\"Return a list of particles which have been resampled, proportional\n",
    "           to the given weights.\"\"\"\n",
    "        new_particles = []\n",
    "        max_weight = max(weights)\n",
    "        index = random.randint(0, len(self.particles) - 1)\n",
    "        offset = 0.0\n",
    "        for i in range(len(self.particles)):\n",
    "            offset += random.uniform(0, 2.0 * max_weight)\n",
    "            while offset > weights[index]:\n",
    "                offset -= weights[index]\n",
    "                index = (index + 1) % len(weights)\n",
    "            new_particles.append(copy.deepcopy(self.particles[index]))\n",
    "        return new_particles\n",
    "\n",
    "    def correct(self, cylinders):\n",
    "        \"\"\"The correction step of FastSLAM.\"\"\"\n",
    "        # Update all particles and compute their weights.\n",
    "        weights = self.update_and_compute_weights(cylinders)\n",
    "        # Then resample, based on the weight array.\n",
    "        self.particles = self.resample(weights)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Robot constants.\n",
    "    scanner_displacement = 30.0\n",
    "    ticks_to_mm = 0.349\n",
    "    robot_width = 155.0\n",
    "\n",
    "    # Cylinder extraction and matching constants.\n",
    "    minimum_valid_distance = 20.0\n",
    "    depth_jump = 100.0\n",
    "    cylinder_offset = 90.0\n",
    "\n",
    "    # Filter constants.\n",
    "    control_motion_factor = 0.35  # Error in motor control.\n",
    "    control_turn_factor = 0.6  # Additional error due to slip when turning.\n",
    "    measurement_distance_stddev = 200.0  # Distance measurement error of cylinders.\n",
    "    measurement_angle_stddev = 15.0 / 180.0 * pi  # Angle measurement error.\n",
    "    minimum_correspondence_likelihood = 0.001  # Min likelihood of correspondence.\n",
    "\n",
    "    # Generate initial particles. Each particle is (x, y, theta).\n",
    "    number_of_particles = 25\n",
    "    start_state = np.array([500.0, 0.0, 45.0 / 180.0 * pi])\n",
    "    initial_particles = [ Particle_5(start_state.copy())\n",
    "                          for _ in range(number_of_particles)]\n",
    "\n",
    "    # Setup filter.\n",
    "    fs = FastSLAM_2(initial_particles,\n",
    "                    robot_width, scanner_displacement,\n",
    "                    control_motion_factor, control_turn_factor,\n",
    "                    measurement_distance_stddev,\n",
    "                    measurement_angle_stddev,\n",
    "                    minimum_correspondence_likelihood)\n",
    "\n",
    "    # Read data.\n",
    "    logfile = LegoLogfile()\n",
    "    logfile.read(\"robot4_motors.txt\")\n",
    "    logfile.read(\"robot4_scan.txt\")\n",
    "\n",
    "    # Loop over all motor tick records.\n",
    "    # This is the FastSLAM filter loop, with prediction and correction.\n",
    "    f = open(\"fast_slam_correction.txt\", \"w\")\n",
    "    for i in range(len(logfile.motor_ticks)):\n",
    "        # Prediction.\n",
    "        control = [x * ticks_to_mm for x in logfile.motor_ticks[i]]\n",
    "        fs.predict(control)\n",
    "\n",
    "        # Correction.\n",
    "        cylinders = get_cylinders_from_scan(logfile.scan_data[i], depth_jump,\n",
    "            minimum_valid_distance, cylinder_offset)\n",
    "        fs.correct(cylinders)\n",
    "\n",
    "        # Output particles.\n",
    "        print_particles(fs.particles, f)\n",
    "\n",
    "        # Output state estimated from all particles.\n",
    "        mean = get_mean(fs.particles)\n",
    "        print(\"F %.0f %.0f %.3f\" %\\\n",
    "              (mean[0] + scanner_displacement * cos(mean[2]),\n",
    "               mean[1] + scanner_displacement * sin(mean[2]),\n",
    "               mean[2]), file=f)\n",
    "\n",
    "        # Output error ellipse and standard deviation of heading.\n",
    "        errors = get_error_ellipse_and_heading_variance(fs.particles, mean)\n",
    "        print(\"E %.3f %.0f %.0f %.3f\" % errors, file=f)\n",
    "\n",
    "        # Output landmarks of particle which is closest to the mean position.\n",
    "        output_particle = min([\n",
    "            (np.linalg.norm(mean[0:2] - fs.particles[i].pose[0:2]),i)\n",
    "            for i in range(len(fs.particles)) ])[1]\n",
    "        # Write estimates of landmarks.\n",
    "        write_cylinders(f, \"W C\",\n",
    "                        fs.particles[output_particle].landmark_positions)\n",
    "        # Write covariance matrices.\n",
    "        write_error_ellipses(f, \"W E\",\n",
    "                             fs.particles[output_particle].landmark_covariances)\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7694f252e384b7ead06e11af19c2d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Output(layout=Layout(width='600px')), Output(layout=Layout(width='600px')))), Ou…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute this to run the interactive viewer.\n",
    "import ipy_logfile_viewer as lfv\n",
    "v = lfv.IPYLogfileViewer(files=[\"fast_slam_correction.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cb2bfc876b0277be428cf961db9fce1",
     "grade": false,
     "grade_id": "cell-0a422947fff52d65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Removing spurious landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8bdbeff7e2fb9fc5fa143d41b8720c45",
     "grade": false,
     "grade_id": "cell-49d4991000f8bb8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"//av.tib.eu/player/49037\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd704e23790>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't see a video below, run this cell.\n",
    "IFrame(\"https://www.youtube.com/embed/1lXD7nHDBxg\" if \"YouTube\" in globals() else \"//av.tib.eu/player/49037\",\n",
    "       width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1be6f77d38fc11954a81a13caed68984",
     "grade": false,
     "grade_id": "cell-3159d6b723a36844",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Programming assignment: modify the previous code to remove spurious landmarks (10 Points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51239cf7ae77eda3e6478046afcca54d",
     "grade": false,
     "grade_id": "cell-6173610b8850f9f1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# slam_10_f_counter\n",
    "from lego_robot import *\n",
    "from slam_g_library import get_cylinders_from_scan, write_cylinders,\\\n",
    "     write_error_ellipses, get_mean, get_error_ellipse_and_heading_variance,\\\n",
    "     print_particles\n",
    "from math import sin, cos, pi\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Particle(Particle_5):\n",
    "    def __init__(self, pose):\n",
    "        super().__init__(pose)\n",
    "        self.landmark_counters = []  # Added: counter for each landmark.\n",
    "\n",
    "    # Solution from above, but modification is required.\n",
    "    def update_particle(self, measurement, measurement_in_scanner_system,\n",
    "                        number_of_landmarks,\n",
    "                        minimum_correspondence_likelihood,\n",
    "                        Qt_measurement_covariance, scanner_displacement):\n",
    "        \"\"\"Given a measurement, computes the likelihood that it belongs to any\n",
    "           of the landmarks in the particle. If there are none, or if all\n",
    "           likelihoods are below the minimum_correspondence_likelihood\n",
    "           threshold, add a landmark to the particle. Otherwise, update the\n",
    "           (existing) landmark with the largest likelihood.\"\"\"\n",
    "        # --->>> Copy & paste your code from above here.\n",
    "        #        Then, modify as follows:\n",
    "        #        - If a new landmark is initialized, append 1 to\n",
    "        #          landmark_counters.\n",
    "        #        - If an existing landmark is updated, add 2 to the\n",
    "        #          corresponding landmark counter.\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        likelihoods = self.compute_correspondence_likelihoods(measurement, \n",
    "                                                              number_of_landmarks, \n",
    "                                                              Qt_measurement_covariance,\n",
    "                                                              scanner_displacement)\n",
    "                         # compute_correspondence_likelihoods().\n",
    "\n",
    "        # If the likelihood list is empty, or the max correspondence likelihood\n",
    "        # is still smaller than minimum_correspondence_likelihood, setup\n",
    "        # a new landmark.\n",
    "        if not likelihoods or\\\n",
    "           max(likelihoods) < minimum_correspondence_likelihood:\n",
    "            # --->>> Add code to insert a new landmark.\n",
    "            self.initialize_new_landmark(measurement_in_scanner_system,\n",
    "                                         Qt_measurement_covariance,\n",
    "                                         scanner_displacement)\n",
    "            self.landmark_counters.append(1)\n",
    "            return minimum_correspondence_likelihood\n",
    "\n",
    "        # Else update the particle's EKF for the corresponding particle.\n",
    "        else:\n",
    "            # This computes (max, argmax) of measurement_likelihoods.\n",
    "\n",
    "            # --->>> Add code to find w, the maximum likelihood,\n",
    "            # and the corresponding landmark index.\n",
    "            w = max(likelihoods)\n",
    "            index = likelihoods.index(w)\n",
    "            # Add code to update_landmark().\n",
    "            self.update_landmark(index, \n",
    "                                 measurement, \n",
    "                                 Qt_measurement_covariance, \n",
    "                                 scanner_displacement)\n",
    "            self.landmark_counters[index] += 2\n",
    "            return w\n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    # New: Counter decrement for visible landmarks.\n",
    "    def decrement_visible_landmark_counters(self, scanner_displacement):\n",
    "        \"\"\"Decrements the counter for every landmark which is potentially\n",
    "           visible. This uses a simplified test: it is only checked if the\n",
    "           bearing of the expected measurement is within the laser scanners\n",
    "           range.\"\"\"\n",
    "        # Hints:\n",
    "        # - We only check the bearing angle of the landmarks.\n",
    "        # - Min and max bearing can be obtained from\n",
    "        #   LegoLogfile.min_max_bearing()\n",
    "        # - The bearing for any landmark can be computed using\n",
    "        #   h_expected_measurement_for_landmark()\n",
    "        # - If the bearing is within the range, decrement the corresponding\n",
    "        #   self.landmark_counters[].\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        bearing_min, bearing_max = LegoLogfile.min_max_bearing()\n",
    "        for landmark_number in range(self.number_of_landmarks()):\n",
    "            z_expected = self.h_expected_measurement_for_landmark(landmark_number, scanner_displacement)\n",
    "            if (z_expected[1] >= bearing_min and z_expected[1] <= bearing_max):\n",
    "                self.landmark_counters[landmark_number] -= 1\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    # New: Removal of landmarks with negative counter.\n",
    "    def remove_spurious_landmarks(self):\n",
    "        \"\"\"Remove all landmarks which have a counter less than zero.\"\"\"\n",
    "        # Remove any landmark for which the landmark_counters[] is (strictly)\n",
    "        # smaller than zero.\n",
    "        # Note: deleting elements of a list while iterating over the list\n",
    "        # will not work properly. One relatively simple and elegant solution is\n",
    "        # to make a new list which contains all required elements.\n",
    "        # (A compact way to do this is \"list comprehensions with if clause\",\n",
    "        # but you may also use a for loop).\n",
    "        # Remember to process landmark_positions, landmark_covariances and\n",
    "        # landmark_counters.\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        lm_positions_new = []\n",
    "        lm_covariances_new = []\n",
    "        lm_counters_new = []\n",
    "        \n",
    "        for landmark_number in range(self.number_of_landmarks()):\n",
    "            if (self.landmark_counters[landmark_number] >= 0):\n",
    "                lm_positions_new.append(self.landmark_positions[landmark_number])\n",
    "                lm_covariances_new.append(self.landmark_covariances[landmark_number])\n",
    "                lm_counters_new.append(self.landmark_counters[landmark_number])\n",
    "\n",
    "        self.landmark_positions = lm_positions_new\n",
    "        self.landmark_covariances = lm_covariances_new  \n",
    "        self.landmark_counters = lm_counters_new\n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "\n",
    "class FastSLAM(FastSLAM_2):\n",
    "    # Modification of previous update_and_compute_weights() function.\n",
    "    def update_and_compute_weights(self, cylinders):\n",
    "        \"\"\"Updates all particles and returns a list of their weights.\"\"\"\n",
    "        Qt_measurement_covariance = \\\n",
    "            np.diag([self.measurement_distance_stddev**2,\n",
    "                     self.measurement_angle_stddev**2])\n",
    "        weights = []\n",
    "        for p in self.particles:\n",
    "            # Added: decrement landmark counter for any visible landmark.\n",
    "            p.decrement_visible_landmark_counters(self.scanner_displacement)\n",
    "\n",
    "            # Loop over all measurements.\n",
    "            number_of_landmarks = p.number_of_landmarks()\n",
    "            weight = 1.0\n",
    "            for measurement, measurement_in_scanner_system in cylinders:\n",
    "                weight *= p.update_particle(\n",
    "                    measurement, measurement_in_scanner_system,\n",
    "                    number_of_landmarks,\n",
    "                    self.minimum_correspondence_likelihood,\n",
    "                    Qt_measurement_covariance, self.scanner_displacement)\n",
    "\n",
    "            # Append overall weight of this particle to weight list.\n",
    "            weights.append(weight)\n",
    "            \n",
    "            # Added: remove spurious landmarks (with negative counter).\n",
    "            p.remove_spurious_landmarks()\n",
    "\n",
    "        return weights\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Robot constants.\n",
    "    scanner_displacement = 30.0\n",
    "    ticks_to_mm = 0.349\n",
    "    robot_width = 155.0\n",
    "\n",
    "    # Cylinder extraction and matching constants.\n",
    "    minimum_valid_distance = 20.0\n",
    "    depth_jump = 100.0\n",
    "    cylinder_offset = 90.0\n",
    "\n",
    "    # Filter constants.\n",
    "    control_motion_factor = 0.35  # Error in motor control.\n",
    "    control_turn_factor = 0.6  # Additional error due to slip when turning.\n",
    "    measurement_distance_stddev = 200.0  # Distance measurement error of cylinders.\n",
    "    measurement_angle_stddev = 15.0 / 180.0 * pi  # Angle measurement error.\n",
    "    minimum_correspondence_likelihood = 0.001  # Min likelihood of correspondence.\n",
    "\n",
    "    # Generate initial particles. Each particle is (x, y, theta).\n",
    "    number_of_particles = 25\n",
    "    start_state = np.array([500.0, 0.0, 45.0 / 180.0 * pi])\n",
    "    initial_particles = [ Particle(start_state.copy())\n",
    "                          for _ in range(number_of_particles)]\n",
    "\n",
    "    # Setup filter.\n",
    "    fs = FastSLAM(initial_particles,\n",
    "                  robot_width, scanner_displacement,\n",
    "                  control_motion_factor, control_turn_factor,\n",
    "                  measurement_distance_stddev,\n",
    "                  measurement_angle_stddev,\n",
    "                  minimum_correspondence_likelihood)\n",
    "\n",
    "    # Read data.\n",
    "    logfile = LegoLogfile()\n",
    "    logfile.read(\"robot4_motors.txt\")\n",
    "    logfile.read(\"robot4_scan.txt\")\n",
    "\n",
    "    # Loop over all motor tick records.\n",
    "    # This is the FastSLAM filter loop, with prediction and correction.\n",
    "    f = open(\"fast_slam_counter.txt\", \"w\")\n",
    "    for i in range(len(logfile.motor_ticks)):\n",
    "        # Prediction.\n",
    "        control = [x * ticks_to_mm for x in logfile.motor_ticks[i]]\n",
    "        fs.predict(control)\n",
    "\n",
    "        # Correction.\n",
    "        cylinders = get_cylinders_from_scan(logfile.scan_data[i], depth_jump,\n",
    "            minimum_valid_distance, cylinder_offset)\n",
    "        fs.correct(cylinders)\n",
    "\n",
    "        # Output particles.\n",
    "        print_particles(fs.particles, f)\n",
    "\n",
    "        # Output state estimated from all particles.\n",
    "        mean = get_mean(fs.particles)\n",
    "        print(\"F %.0f %.0f %.3f\" %\\\n",
    "              (mean[0] + scanner_displacement * cos(mean[2]),\n",
    "               mean[1] + scanner_displacement * sin(mean[2]),\n",
    "               mean[2]), file=f)\n",
    "\n",
    "        # Output error ellipse and standard deviation of heading.\n",
    "        errors = get_error_ellipse_and_heading_variance(fs.particles, mean)\n",
    "        print(\"E %.3f %.0f %.0f %.3f\" % errors, file=f)\n",
    "\n",
    "        # Output landmarks of particle which is closest to the mean position.\n",
    "        output_particle = min([\n",
    "            (np.linalg.norm(mean[0:2] - fs.particles[i].pose[0:2]),i)\n",
    "            for i in range(len(fs.particles)) ])[1]\n",
    "        # Write estimates of landmarks.\n",
    "        write_cylinders(f, \"W C\",\n",
    "                        fs.particles[output_particle].landmark_positions)\n",
    "        # Write covariance matrices.\n",
    "        write_error_ellipses(f, \"W E\",\n",
    "                             fs.particles[output_particle].landmark_covariances)\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c8bf6804b24dea9a1370dac51615ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Output(layout=Layout(width='600px')), Output(layout=Layout(width='600px')))), Ou…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute this to run the interactive viewer.\n",
    "import ipy_logfile_viewer as lfv\n",
    "v = lfv.IPYLogfileViewer(files=[\"fast_slam_counter.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24bfc876bc8f2bd6fbbae234b24f53fd",
     "grade": true,
     "grade_id": "cell-47a8d6da5e5426ce",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from lego_robot import *\n",
    "from slam_g_library import get_cylinders_from_scan, get_mean\n",
    "from math import sin, cos, pi\n",
    "import numpy as np\n",
    "def generate_poses(the_particle_class, the_filter_class, n_particles):\n",
    "\n",
    "    # Robot constants.\n",
    "    scanner_displacement = 30.0\n",
    "    ticks_to_mm = 0.349\n",
    "    robot_width = 155.0\n",
    "\n",
    "    # Cylinder extraction and matching constants.\n",
    "    minimum_valid_distance = 20.0\n",
    "    depth_jump = 100.0\n",
    "    cylinder_offset = 90.0\n",
    "\n",
    "    # Filter constants.\n",
    "    control_motion_factor = 0.35\n",
    "    control_turn_factor = 0.6\n",
    "    measurement_distance_stddev = 200.0\n",
    "    measurement_angle_stddev = 15.0 / 180.0 * pi\n",
    "    minimum_correspondence_likelihood = 0.001\n",
    "\n",
    "    # Generate initial particles. Each particle is (x, y, theta).\n",
    "    number_of_particles = n_particles\n",
    "    start_state = np.array([500.0, 0.0, 45.0 / 180.0 * pi])\n",
    "    initial_particles = [ the_particle_class(start_state.copy())\n",
    "                          for _ in range(number_of_particles)]\n",
    "    # Setup filter.\n",
    "    fs = the_filter_class(initial_particles,\n",
    "                  robot_width, scanner_displacement,\n",
    "                  control_motion_factor, control_turn_factor,\n",
    "                  measurement_distance_stddev,\n",
    "                  measurement_angle_stddev,\n",
    "                  minimum_correspondence_likelihood)\n",
    "    # Read data.\n",
    "    logfile = LegoLogfile()\n",
    "    logfile.read(\"robot4_motors.txt\")\n",
    "    logfile.read(\"robot4_scan.txt\")\n",
    "    # Loop over all motor tick records.\n",
    "    poses = []\n",
    "    for i in range(len(logfile.motor_ticks)):\n",
    "        # Prediction.\n",
    "        control = [x * ticks_to_mm for x in logfile.motor_ticks[i]]\n",
    "        fs.predict(control)\n",
    "        # Correction.\n",
    "        cylinders = get_cylinders_from_scan(logfile.scan_data[i], depth_jump,\n",
    "            minimum_valid_distance, cylinder_offset)\n",
    "        fs.correct(cylinders)\n",
    "        mean = get_mean(fs.particles)\n",
    "        poses.append(mean)\n",
    "    return poses\n",
    "\n",
    "def test(the_particle_class, the_filter_class):\n",
    "    ref_poses = np.array([\n",
    "500,0,0.785,500,0,0.785,500,0,0.785,500,0,0.785,500,0,0.785,500,0,0.785,\n",
    "500,0,0.785,500,0,0.785,500,0,0.785,500,0,0.785,500,0,0.785,500,0,0.785,\n",
    "500,0,0.785,517,17,0.786,548,48,0.779,578,77,0.781,608,107,0.780,607,106,0.779,\n",
    "637,135,0.773,668,165,0.770,698,194,0.770,725,221,0.773,723,219,0.770,\n",
    "755,249,0.768,786,281,0.793,787,282,0.790,819,314,0.793,848,343,0.778,\n",
    "878,373,0.770,909,402,0.778,940,431,0.791,940,432,0.787,973,462,0.774,\n",
    "1003,492,0.782,1035,523,0.785,1034,522,0.783,1065,553,0.788,1096,584,0.785,\n",
    "1127,614,0.786,1157,642,0.780,1156,638,0.775,1187,666,0.780,1217,694,0.782,\n",
    "1247,722,0.772,1246,721,0.776,1278,751,0.762,1308,779,0.772,1338,806,0.770,\n",
    "1369,835,0.777,1369,834,0.767,1400,860,0.752,1431,891,0.759,1459,920,0.762,\n",
    "1459,919,0.761,1492,951,0.753,1523,978,0.737,1554,1008,0.732,1554,1007,0.726,\n",
    "1584,1035,0.717,1616,1063,0.722,1644,1090,0.730,1676,1117,0.757,1676,1116,0.741,\n",
    "1705,1147,0.852,1734,1183,0.956,1757,1219,1.044,1776,1259,1.151,1778,1258,1.146,\n",
    "1793,1299,1.223,1804,1342,1.335,1809,1384,1.463,1813,1427,1.563,1815,1426,1.566,\n",
    "1816,1467,1.747,1809,1506,1.930,1797,1542,2.069,1804,1545,2.002,1790,1580,2.096,\n",
    "1778,1613,2.176,1756,1644,2.302,1757,1645,2.287,1729,1674,2.444,1699,1697,2.533,\n",
    "1665,1715,2.668,1632,1731,2.793,1632,1728,2.794,1592,1740,2.947,1552,1747,3.060,\n",
    "1514,1748,-3.083,1509,1746,-3.059,1467,1747,-3.024,1425,1742,-2.967,\n",
    "1384,1731,-2.926,1345,1718,-2.890,1346,1712,-2.883,1306,1700,-2.880,\n",
    "1264,1688,-2.875,1227,1679,-2.872,1186,1670,-2.873,1186,1671,-2.877,\n",
    "1144,1660,-2.888,1105,1649,-2.892,1067,1637,-2.871,1064,1638,-2.880,\n",
    "1021,1627,-2.878,982,1615,-2.880,942,1604,-2.886,906,1592,-2.871,\n",
    "904,1592,-2.879,865,1582,-2.886,822,1571,-2.856,781,1561,-2.783,782,1562,-2.798,\n",
    "745,1544,-2.638,711,1522,-2.506,682,1500,-2.429,682,1498,-2.409,655,1469,-2.271,\n",
    "631,1438,-2.144,611,1408,-2.077,597,1377,-1.941,599,1375,-1.902,593,1339,-1.716,\n",
    "594,1304,-1.569,598,1268,-1.402,600,1262,-1.368,612,1222,-1.181,630,1188,-1.044,\n",
    "650,1159,-0.905,670,1136,-0.830,674,1135,-0.775,711,1107,-0.659,747,1084,-0.580,\n",
    "784,1061,-0.518,788,1060,-0.516,829,1039,-0.504,869,1018,-0.489,909,999,-0.485,\n",
    "910,998,-0.487,949,977,-0.480,988,956,-0.479,1027,934,-0.483,1068,913,-0.479,\n",
    "1105,894,-0.474,1107,892,-0.479,1148,871,-0.484,1188,851,-0.475,1227,832,-0.458,\n",
    "1225,832,-0.470,1265,812,-0.466,1297,794,-0.421,1329,783,-0.307,1352,776,-0.240,\n",
    "1349,776,-0.216,1380,773,0.010,1411,774,0.207,1438,785,0.451,1436,785,0.471,\n",
    "1461,803,0.693,1481,823,0.851,1496,845,0.999,1511,869,1.214,1508,867,1.253,\n",
    "1516,899,1.510,1519,929,1.648,1514,969,1.747,1510,967,1.818,1496,1010,1.894,\n",
    "1477,1048,1.970,1459,1084,2.006,1441,1119,2.017,1439,1122,2.016,1421,1161,2.018,\n",
    "1403,1202,2.019,1384,1241,2.020,1366,1273,2.021,1366,1273,2.012,1348,1311,2.052,\n",
    "1329,1348,2.089,1308,1385,2.178,1307,1385,2.190,1277,1422,2.281,1245,1455,2.369,\n",
    "1215,1484,2.455,1217,1485,2.459,1181,1511,2.558,1144,1533,2.667,1107,1550,2.773,\n",
    "1068,1563,2.900,1066,1564,2.918,1027,1570,3.049,991,1571,-3.133,953,1568,-3.053,\n",
    "911,1560,-2.950,908,1558,-2.907,867,1544,-2.859,825,1529,-2.792,784,1511,-2.741,\n",
    "783,1511,-2.729,745,1492,-2.720,703,1472,-2.717,666,1455,-2.710,627,1438,-2.718,\n",
    "625,1438,-2.719,588,1419,-2.717,550,1403,-2.686,512,1383,-2.657,513,1381,-2.649,\n",
    "475,1359,-2.559,439,1332,-2.470,407,1305,-2.428,407,1305,-2.429,373,1276,-2.286,\n",
    "346,1241,-2.154,323,1206,-2.046,305,1171,-1.954,304,1170,-1.949,289,1126,-1.882,\n",
    "276,1084,-1.837,268,1039,-1.718,261,994,-1.674,260,988,-1.673,256,939,-1.578,\n",
    "258,892,-1.499,261,848,-1.420,266,844,-1.404,276,796,-1.314,290,750,-1.205,\n",
    "309,708,-1.096,328,673,-1.033,329,671,-1.032,356,633,-0.933,384,600,-0.866,\n",
    "414,567,-0.795,414,568,-0.784,446,539,-0.694,482,513,-0.597,516,491,-0.539,\n",
    "553,472,-0.473,554,472,-0.455,595,455,-0.371,641,441,-0.321,684,429,-0.303,\n",
    "687,430,-0.303,730,417,-0.306,773,404,-0.308,817,391,-0.310,857,378,-0.295,\n",
    "856,377,-0.304,899,363,-0.258,941,354,-0.214,980,345,-0.150,979,345,-0.142,\n",
    "1024,341,-0.036,1069,340,0.028,1110,345,0.074,1152,349,0.091,1150,349,0.090,\n",
    "1196,353,0.086,1239,356,0.095,1282,361,0.102,1284,358,0.091,1329,362,0.088,\n",
    "1372,365,0.092,1414,369,0.087,1415,369,0.094,1461,373,0.083,1505,376,0.089,\n",
    "1545,380,0.098,1585,386,0.106,1591,384,0.098,1592,383,0.085,1594,382,0.077,\n",
    "1595,382,0.075,1595,382,0.073,1595,382,0.072,1595,382,0.072,1594,382,0.071,])\n",
    "    ref_poses = ref_poses.reshape(ref_poses.shape[0]//3,3)\n",
    "\n",
    "    for experiment in range(10):\n",
    "        # Generate poses.\n",
    "        poses = np.array(generate_poses(the_particle_class,\n",
    "                                        the_filter_class, 25))\n",
    "        # Compare.\n",
    "        diff = ref_poses-poses\n",
    "        diff[:,2] = (diff[:,2] + np.pi) % (2*np.pi) - np.pi\n",
    "        d = np.max(np.abs(diff), axis=0)\n",
    "\n",
    "        # Quit if close.\n",
    "        if (d < (150, 150, 0.3)).all():\n",
    "            return True\n",
    "    return False\n",
    "assert test(Particle, FastSLAM), \"Oh no, it's wrong!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59f7aa55658a748ae0b9850ab80d361c",
     "grade": false,
     "grade_id": "cell-86af570493140618",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Conclusions and final remarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "178ea0c22551a9226a25688a7ee8ea35",
     "grade": false,
     "grade_id": "cell-6ac74cb092fd6c4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"//av.tib.eu/player/49038\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd708976f90>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't see a video below, run this cell.\n",
    "IFrame(\"https://www.youtube.com/embed/gwxBrE-aYmc\" if \"YouTube\" in globals() else \"//av.tib.eu/player/49038\",\n",
    "       width=560, height=315)"
   ]
  }
 ],
 "metadata": {
  "copyright": "(c) Claus Brenner 2021",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
